{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd3e648",
   "metadata": {},
   "source": [
    "# SPARC-P PubApps Deployment Notebook\n",
    "\n",
    "This notebook is the runnable version of Step 4 for deploying SPARC-P to UF RC PubApps.\n",
    "\n",
    "## Resource Profiles\n",
    "- **HiPerGator (parallel jobs)**: 4 GPUs, 16 CPU cores\n",
    "- **PubApps (serving)**: 1x L4 GPU (24GB), 2 CPU cores, 16GB RAM\n",
    "\n",
    "## Before You Run\n",
    "- You are on your PubApps VM via SSH\n",
    "- You have your project account (`SPARCP`)\n",
    "- Trained models are available from HiPerGator at `/blue/jasondeanarnold/SPARCP/trained_models`\n",
    "- Podman + systemd user services are available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce41b8b",
   "metadata": {},
   "source": [
    "All paths, hostnames, and resource limits used throughout this notebook are defined here. Nothing is deployed yet — this section only establishes the variables.\n",
    "\n",
    "What gets defined and why:\n",
    "- **Paths** (`PUBAPPS_ROOT`, `MODEL_DIR`, `BACKEND_DIR`, `CONDA_ENV`, `RIVA_MODEL_DIR`): All point to the `/pubapps/SPARCP/` directory tree on the PubApps VM. These are used by every other cell to know where to create directories, install the conda environment, and write files.\n",
    "- **Source paths** (`BASE_PATH`, `HIPERGATOR_SOURCE_MODELS`): The `/blue/jasondeanarnold/SPARCP/trained_models` location on HiPerGator where the fine-tuned model adapters live. Used to construct the `rsync` transfer command.\n",
    "- **Connection info** (`PUBAPPS_HOST`, `PUBAPPS_SSH_USER`): The hostname and SSH username for the PubApps VM, used in the model transfer step.\n",
    "- **CORS origins**: The two allowed domains (`hpvcommunicationtraining.com` and `.org`) that the Unity app runs on. Only these origins can make API calls to the backend — all others are rejected by the server.\n",
    "- **Firebase path**: Where the Firebase service account credentials file lives on the PubApps VM. Required by the backend to write session state to Firestore.\n",
    "- **Resource constants** (`PUBAPPS_GPU`, `PUBAPPS_CORES`, `PUBAPPS_RAM_GB`, `UVICORN_WORKERS`): Hard documented limits for the L4 GPU (24 GB VRAM), 2 CPU cores, and 16 GB RAM PubApps environment. `UVICORN_WORKERS = 1` is intentional — with only 2 cores and the LLM holding most GPU memory, running more than 1 worker would cause out-of-memory errors.\n",
    "\n",
    "> **All values can be overridden** by setting environment variables before running this notebook (e.g., `export SPARC_PUBAPPS_PROJECT=MYPROJECT`).  The printed output at the bottom confirms the resolved values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ce951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "import os\n",
    "import subprocess\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT = os.environ.get(\"SPARC_PUBAPPS_PROJECT\", \"SPARCP\")\n",
    "PUBAPPS_ROOT = Path(f\"/pubapps/{PROJECT}\")\n",
    "MODEL_DIR = PUBAPPS_ROOT / \"models\"\n",
    "CONDA_ENV = PUBAPPS_ROOT / \"conda_envs\" / \"sparc_backend\"\n",
    "BACKEND_DIR = PUBAPPS_ROOT / \"backend\"\n",
    "RIVA_MODEL_DIR = PUBAPPS_ROOT / \"riva_models\"\n",
    "\n",
    "BASE_PATH = os.environ.get(\"SPARC_BASE_PATH\", \"/blue/jasondeanarnold/SPARCP\")\n",
    "HIPERGATOR_SOURCE_MODELS = os.environ.get(\n",
    "    \"SPARC_HIPERGATOR_SOURCE_MODELS\",\n",
    "    f\"{BASE_PATH}/trained_models\",\n",
    ")\n",
    "PUBAPPS_HOST = os.environ.get(\"SPARC_PUBAPPS_HOST\", \"pubapps-vm.rc.ufl.edu\")\n",
    "PUBAPPS_SSH_USER = os.environ.get(\"SPARC_PUBAPPS_SSH_USER\", PROJECT)\n",
    "PUBAPP_ALLOWED_ORIGINS = os.environ.get(\n",
    "    \"SPARC_CORS_ALLOWED_ORIGINS\",\n",
    "    \"https://hpvcommunicationtraining.com,https://hpvcommunicationtraining.org\",\n",
    ")\n",
    "FIREBASE_CREDS_PATH = Path(\n",
    "    os.environ.get(\"SPARC_FIREBASE_CREDS\", str(PUBAPPS_ROOT / \"config\" / \"firebase-credentials.json\"))\n",
    ")\n",
    "\n",
    "# Resource constraints\n",
    "HPG_MAX_GPUS = 4\n",
    "HPG_MAX_CORES = 16\n",
    "PUBAPPS_GPU = \"L4 (24GB)\"\n",
    "PUBAPPS_CORES = 2\n",
    "PUBAPPS_RAM_GB = 16\n",
    "UVICORN_WORKERS = 1  # tuned for 2 CPU cores and 16GB RAM\n",
    "\n",
    "print(f\"Project: {PROJECT}\")\n",
    "print(f\"PubApps root: {PUBAPPS_ROOT}\")\n",
    "print(f\"Conda env: {CONDA_ENV}\")\n",
    "print(f\"Backend dir: {BACKEND_DIR}\")\n",
    "print(f\"HiPerGator source models: {HIPERGATOR_SOURCE_MODELS}\")\n",
    "print(f\"PubApps host: {PUBAPPS_HOST}\")\n",
    "print(f\"PubApps SSH user: {PUBAPPS_SSH_USER}\")\n",
    "print(f\"Allowed CORS origins: {PUBAPP_ALLOWED_ORIGINS}\")\n",
    "print(f\"Firebase creds path: {FIREBASE_CREDS_PATH}\")\n",
    "print(f\"HiPerGator resources: {HPG_MAX_GPUS} GPUs, {HPG_MAX_CORES} cores\")\n",
    "print(f\"PubApps resources: {PUBAPPS_GPU}, {PUBAPPS_CORES} cores, {PUBAPPS_RAM_GB}GB RAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6000b",
   "metadata": {},
   "source": [
    "The `run()` helper function used throughout this notebook executes shell commands — with a critical safety mechanism built in: by default, **nothing actually runs**.\n",
    "\n",
    "The `EXECUTE = False` flag at the top means this notebook is in \"dry-run\" mode. When you call `run(\"some command\")`, it prints `$ some command` followed by `(dry-run) command not executed` — showing you exactly what *would* happen without actually doing it.\n",
    "\n",
    "To deploy for real, change `EXECUTE = False` to `EXECUTE = True` and re-run all cells from top to bottom.\n",
    "\n",
    "Why this pattern is useful:\n",
    "- You can review the full deployment sequence (what commands would run, in what order, with what paths) before committing to any changes on the PubApps VM.\n",
    "- It prevents accidental partial deployments if you're just opening this notebook to check something.\n",
    "- The `check=True` default means that when `EXECUTE = True`, any command that returns a non-zero exit code (error) raises a `RuntimeError` immediately, stopping the notebook rather than silently continuing with a broken state. A few commands use `check=False` when their failure is non-fatal (e.g., status checks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Command runner (safe by default)\n",
    "EXECUTE = False  # Set True to actually run shell commands\n",
    "\n",
    "def run(cmd: str, check: bool = True):\n",
    "    print(f\"$ {cmd}\")\n",
    "    if not EXECUTE:\n",
    "        print(\"(dry-run) command not executed\\n\")\n",
    "        return None\n",
    "    result = subprocess.run([\"bash\", \"-lc\", cmd], capture_output=True, text=True)\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(result.stderr)\n",
    "    if check and result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
    "    print()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6cdeb",
   "metadata": {},
   "source": [
    "## 3. Transfer Models from HiPerGator\n",
    "Run this on HiPerGator or from a hop host with access to both systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d448c",
   "metadata": {},
   "source": [
    "The `rsync` command below transfers the fine-tuned SPARC-P model adapters from HiPerGator's `/blue` storage to the PubApps VM's model directory. The command is **printed, not executed** (unless `EXECUTE = True`).\n",
    "\n",
    "`rsync` is the right tool for this task because:\n",
    "- It transfers only files that have changed (delta sync), so re-running after a partial transfer is safe and fast.\n",
    "- The `-avz` flags mean: archive mode (preserves permissions and timestamps), verbose output, and gzip compression in transit.\n",
    "- `--progress` shows a progress bar for each file, which is helpful since the fine-tuned adapters can be 2–10 GB total.\n",
    "\n",
    "The command transfers from `HIPERGATOR_SOURCE_MODELS/` (the `/blue/jasondeanarnold/SPARCP/trained_models` directory) to `MODEL_DIR` on the PubApps VM using SSH as the transport.\n",
    "\n",
    "> **Where to run this:** This command needs to run on HiPerGator (where the `/blue` filesystem is mounted) or on a machine that has SSH access to both systems. Copy the printed command and run it in your HiPerGator terminal session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Render model sync command\n",
    "rsync_cmd = textwrap.dedent(f\"\"\"\n",
    "rsync -avz --progress \\\n",
    "  {HIPERGATOR_SOURCE_MODELS}/ \\\n",
    "  {PUBAPPS_SSH_USER}@{PUBAPPS_HOST}:{MODEL_DIR}/\n",
    "\"\"\").strip()\n",
    "print(rsync_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f365d",
   "metadata": {},
   "source": [
    "## 4. PubApps Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbced4f8",
   "metadata": {},
   "source": [
    "All the directory folders the SPARC-P service needs on the PubApps VM are created here before any files are written or software is installed. A single `mkdir -p` command creates the entire directory structure in one shot — the `-p` flag means it creates every level in the path and doesn't fail if a directory already exists.\n",
    "\n",
    "The directories created:\n",
    "- **`/pubapps/SPARCP/`** — the project root for everything SPARC-P on this VM\n",
    "- **`/pubapps/SPARCP/models/`** — where the fine-tuned LLM adapters (CaregiverAgent, CoachAgent, SupervisorAgent) are stored after transfer from HiPerGator\n",
    "- **`/pubapps/SPARCP/backend/`** — where the FastAPI `main.py` application code lives\n",
    "- **`/pubapps/SPARCP/riva_models/`** — where NVIDIA Riva's ASR and TTS model files are stored\n",
    "- **`/pubapps/SPARCP/conda_envs/`** — the parent directory for the `sparc_backend` conda environment\n",
    "\n",
    "This is always the first step before installing software or transferring files — you can't write to a directory that doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89578cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Create required directories\n",
    "run(f\"mkdir -p {PUBAPPS_ROOT} {MODEL_DIR} {BACKEND_DIR} {RIVA_MODEL_DIR} {PUBAPPS_ROOT / 'conda_envs'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab3328",
   "metadata": {},
   "source": [
    "The complete Python software environment for the SPARC-P backend is installed using conda — three commands verify, create, and validate the environment in sequence.\n",
    "\n",
    "Step by step:\n",
    "1. **`conda --version`** (`check=False`) — confirms conda is available on this VM. The `check=False` means a failure here just prints a warning rather than stopping the notebook; useful if you're running in dry-run mode.\n",
    "2. **`conda env create -f environment_backend.yml -p {CONDA_ENV}`** — creates the entire Python environment from a configuration file. The `-f environment_backend.yml` points to the yaml file that lists every package and version (FastAPI, PyTorch, transformers, bitsandbytes, NeMo Guardrails, etc.). The `-p` flag installs the environment to the exact path `/pubapps/SPARCP/conda_envs/sparc_backend` instead of conda's default location. This can take **10–30 minutes** as it downloads and installs hundreds of packages including CUDA-compiled PyTorch.\n",
    "3. **`conda run -p {CONDA_ENV} python -c 'import fastapi,langgraph,torch; print(\"backend env ok\")'`** — immediately tests that the newly created environment is functional by importing three critical libraries. If any import fails, this command fails loudly, catching broken installs before you proceed.\n",
    "\n",
    "> **One-time step:** Only run this if the conda environment doesn't already exist. If it exists and you just want to update it, use `conda env update -f environment_backend.yml -p {CONDA_ENV}` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Create backend conda environment\n",
    "run(\"conda --version\", check=False)\n",
    "run(f\"cd {PUBAPPS_ROOT}; conda env create -f environment_backend.yml -p {CONDA_ENV}\")\n",
    "run(f\"conda run -p {CONDA_ENV} python -c 'import fastapi,langgraph,torch; print(\\\"backend env ok\\\")'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53132751",
   "metadata": {},
   "source": [
    "## 5. Deploy Riva with Podman + Quadlet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd3508",
   "metadata": {},
   "source": [
    "A \"Quadlet\" file tells Podman and systemd how to manage the NVIDIA Riva speech server as a persistent background service on the PubApps VM — similar to how you'd configure a Windows Service, but for Linux.\n",
    "\n",
    "What a Quadlet is: On modern Linux systems, systemd is the service manager. Podman Quadlets are simple configuration files that let systemd start, stop, and automatically restart containers — without needing Docker daemon or complex shell scripts.\n",
    "\n",
    "What the generated `riva-server.container` file specifies:\n",
    "- **`Image`**: Downloads and runs NVIDIA's official Riva speech server container from their registry (NGC) at version 2.16.0.\n",
    "- **`Device=nvidia.com/gpu=all`**: Uses the modern CDI (Container Device Interface) standard to pass the L4 GPU through to the container. This is required for Riva to run its ASR and TTS models.\n",
    "- **`Volume={RIVA_MODEL_DIR}:/data:Z`**: Mounts the local riva_models directory into the container where Riva looks for its model files. The `:Z` sets the correct SELinux label for Podman.\n",
    "- **`PublishPort=50051:50051`**: Exposes Riva's gRPC port so the FastAPI backend (running outside the container) can connect using `localhost:50051`.\n",
    "- **`Restart=always`**: If Riva crashes or the VM reboots, systemd automatically restarts it.\n",
    "- **`TimeoutStartSec=300`**: Gives Riva 5 minutes to start (loading ASR and TTS models into GPU memory takes ~2 minutes).\n",
    "- The file is written to `~/.config/containers/systemd/` — the per-user path where Podman looks for Quadlet definitions.\n",
    "\n",
    "> **The CDI assertion at the end** (`Device=nvidia.com/gpu=all` in `quadlet_content`) is a regression check that confirms the GPU mapping was written correctly before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80caf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Write quadlet service for Riva\n",
    "quadlet_dir = Path.home() / '.config/containers/systemd'\n",
    "quadlet_dir.mkdir(parents=True, exist_ok=True)\n",
    "quadlet_file = quadlet_dir / 'riva-server.container'\n",
    "quadlet_content = textwrap.dedent(f\"\"\"\n",
    "[Unit]\n",
    "Description=SPARC-P Riva Speech Server\n",
    "After=network-online.target\n",
    "\n",
    "[Container]\n",
    "Image=nvcr.io/nvidia/riva/riva-speech:2.16.0-server\n",
    "ContainerName=riva-server\n",
    "Device=nvidia.com/gpu=all\n",
    "Volume={RIVA_MODEL_DIR}:/data:Z\n",
    "PublishPort=50051:50051\n",
    "Environment=NVIDIA_VISIBLE_DEVICES=all\n",
    "Exec=/opt/riva/bin/riva_server --riva_model_repo=/data/models\n",
    "\n",
    "[Service]\n",
    "Restart=always\n",
    "TimeoutStartSec=300\n",
    "\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "\"\"\").strip()\n",
    "quadlet_file.write_text(quadlet_content)\n",
    "print(f\"Wrote {quadlet_file}\")\n",
    "print(quadlet_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28c7336",
   "metadata": {},
   "source": [
    "Downloading the Riva container image and activating Riva as a running systemd service completes the Quadlet setup. Four commands run in sequence, plus a validation check.\n",
    "\n",
    "Step by step:\n",
    "1. **`podman pull nvcr.io/nvidia/riva/riva-speech:2.16.0-server`**: Downloads the Riva container image from NVIDIA's container registry. This is a large image (~5–8 GB) and needs to happen before systemd can start the service. Only needed once.\n",
    "2. **`systemctl --user daemon-reload`**: Tells systemd to re-read all service definitions from disk, including the Quadlet file written in the previous cell. Without this, systemd wouldn't know about the new `riva-server` service.\n",
    "3. **`systemctl --user enable --now riva-server`**: Registers the Riva service to start automatically on login (`enable`) and starts it immediately right now (`--now`). After this command, Riva begins loading its ASR and TTS models into GPU memory.\n",
    "4. **`systemctl --user status riva-server`** (`check=False`): Shows the current status of the Riva service. Expected output: `Active: active (running)`. The `check=False` prevents this from stopping the notebook if the service is still starting.\n",
    "5. **GPU validation commands**: `nvidia-ctk cdi list` confirms the CDI GPU device is registered, and the `podman run ... nvidia-smi` command runs `nvidia-smi` inside a test container to confirm Riva's container can actually see the GPU.\n",
    "6. **Assertion**: Verifies the Quadlet file contains the correct CDI GPU mapping (`Device=nvidia.com/gpu=all`) and not the legacy mapping (`AddDevice=`), which would fail on newer Podman versions.\n",
    "\n",
    "> **Expected next:** Allow 2–3 minutes for Riva to initialize. Check `journalctl --user -u riva-server -n 50` to monitor startup progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73679112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Pull image and enable Riva service\n",
    "run(\"podman pull nvcr.io/nvidia/riva/riva-speech:2.16.0-server\")\n",
    "run(\"systemctl --user daemon-reload\")\n",
    "run(\"systemctl --user enable --now riva-server\")\n",
    "run(\"systemctl --user status riva-server --no-pager\", check=False)\n",
    "run(\"nvidia-ctk cdi list\", check=False)\n",
    "run(\"podman run --rm --device nvidia.com/gpu=all nvidia/cuda:12.8.0-base-ubuntu22.04 nvidia-smi\", check=False)\n",
    "assert \"Device=nvidia.com/gpu=all\" in quadlet_content, \"CDI GPU mapping missing in quadlet\"\n",
    "assert \"AddDevice=\" not in quadlet_content, \"Legacy AddDevice mapping still present in quadlet\"\n",
    "print(\"✅ M10 Quadlet CDI validation passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454afcc",
   "metadata": {},
   "source": [
    "## 6. Create FastAPI Backend + Systemd Service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06042805",
   "metadata": {},
   "source": [
    "This is the most important cell in the notebook — it writes the complete, production-ready `main.py` FastAPI application (approximately 520 lines) to disk at `/pubapps/SPARCP/backend/main.py`. This is the actual program that runs on the PubApps server and handles every interaction with SPARC-P users.\n",
    "\n",
    "What the written application does (plain-English overview of each major section):\n",
    "\n",
    "**System startup (lifespan):** When the server starts, it loads all three fine-tuned LLM adapters (CaregiverAgent, CoachAgent, SupervisorAgent) into GPU memory using 4-bit quantization (NF4 format via bitsandbytes) to fit within the L4's 24 GB VRAM. It also connects to Riva for speech, loads the NeMo Guardrails safety config, and creates the audio file cache directory.\n",
    "\n",
    "**Safety pipeline (guardrails):** Every incoming user message passes through NeMo Guardrails before reaching the AI models. Off-topic messages (politics, finance, anything unrelated to HPV vaccine communication) are rejected with a pre-set refusal message. The AI's response also passes through guardrails before being sent back — a two-stage safety check.\n",
    "\n",
    "**PII redaction (Presidio):** Before any text touches Firebase or logging, it's passed through Microsoft Presidio to redact personal identifiers (names, phone numbers, medical record numbers). If Presidio fails to initialize, all text is replaced with `[REDACTED]` rather than risking PHI exposure — a \"fail-closed\" safety posture.\n",
    "\n",
    "**API authentication:** Every API call requires an `X-API-Key` header. The Unity client sends this key, and the server validates it against the `SPARC_API_KEY` environment variable. This prevents unauthorized access to the backend.\n",
    "\n",
    "**Circuit breakers:** If the LLM, coach, or Riva TTS times out three times in a row, the corresponding circuit \"opens\" for 30 seconds — returning a graceful degraded response instead of queuing more timeout requests. Once 30 seconds pass, the circuit closes and normal operation resumes.\n",
    "\n",
    "**Audio delivery:** Instead of base64-encoding audio in the API response (which would be very large), TTS audio is written to a temp file and returned as a URL (`/v1/audio/{id}`) that expires after 5 minutes. The Unity client fetches the audio separately.\n",
    "\n",
    "**Firebase session state:** After each turn, the session's last message and response (Presidio-redacted) are written to Firestore for session continuity and audit purposes.\n",
    "\n",
    "> **The file is written to disk but the server is not yet started.** The systemd service section below handles starting the running process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22080f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Write backend main.py (integration-ready)\n",
    "main_py = BACKEND_DIR / 'main.py'\n",
    "main_content = textwrap.dedent('''\n",
    "import asyncio\n",
    "import time\n",
    "import tempfile\n",
    "import uuid\n",
    "import os\n",
    "import logging\n",
    "from contextlib import asynccontextmanager\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from fastapi import Depends, FastAPI, Header, HTTPException, status\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import FileResponse, JSONResponse\n",
    "from pydantic import BaseModel, Field\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import riva.client\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "\n",
    "MODEL_BASE_PATH = os.getenv(\"SPARC_MODEL_BASE_PATH\", \"{MODEL_DIR}\")\n",
    "RIVA_SERVER = os.getenv(\"SPARC_RIVA_SERVER\", \"localhost:50051\")\n",
    "FIREBASE_CREDS = os.getenv(\"SPARC_FIREBASE_CREDS\", \"{PUBAPPS_ROOT}/config/firebase-credentials.json\")\n",
    "GUARDRAILS_DIR = os.getenv(\"SPARC_GUARDRAILS_DIR\", os.path.join(os.path.dirname(__file__), \"guardrails\"))\n",
    "\n",
    "API_AUTH_ENABLED = os.getenv(\"SPARC_API_AUTH_ENABLED\", \"true\").strip().lower() == \"true\"\n",
    "API_KEY = os.getenv(\"SPARC_API_KEY\", \"\")\n",
    "CORS_ALLOWED_ORIGINS = [\n",
    "    origin.strip()\n",
    "    for origin in os.getenv(\"SPARC_CORS_ALLOWED_ORIGINS\", \"{PUBAPP_ALLOWED_ORIGINS}\").split(\",\")\n",
    "    if origin.strip()\n",
    "]\n",
    "CORS_ALLOW_CREDENTIALS = os.getenv(\"SPARC_CORS_ALLOW_CREDENTIALS\", \"false\").strip().lower() == \"true\"\n",
    "CORS_ALLOWED_METHODS = [\"GET\", \"POST\", \"OPTIONS\"]\n",
    "CORS_ALLOWED_HEADERS = [\"Content-Type\", \"X-API-Key\", \"Authorization\"]\n",
    "API_CONTRACT_VERSION = \"v1\"\n",
    "LLM_TIMEOUT_SECONDS = float(os.getenv(\"SPARC_LLM_TIMEOUT_SECONDS\", \"10\"))\n",
    "COACH_TIMEOUT_SECONDS = float(os.getenv(\"SPARC_COACH_TIMEOUT_SECONDS\", \"10\"))\n",
    "TTS_TIMEOUT_SECONDS = float(os.getenv(\"SPARC_TTS_TIMEOUT_SECONDS\", \"5\"))\n",
    "TTS_MAX_AUDIO_BYTES = int(os.getenv(\"SPARC_TTS_MAX_AUDIO_BYTES\", \"524288\"))\n",
    "SPARC_AUDIO_URL_TTL_SECONDS = float(os.getenv(\"SPARC_AUDIO_URL_TTL_SECONDS\", \"300\"))\n",
    "SPARC_AUDIO_CACHE_DIR = os.getenv(\"SPARC_AUDIO_CACHE_DIR\", os.path.join(tempfile.gettempdir(), \"sparc_tts_audio\"))\n",
    "CIRCUIT_BREAKER_THRESHOLD = int(os.getenv(\"SPARC_TIMEOUT_CIRCUIT_THRESHOLD\", \"3\"))\n",
    "CIRCUIT_BREAKER_RESET_SECONDS = float(os.getenv(\"SPARC_TIMEOUT_CIRCUIT_RESET_SECONDS\", \"30\"))\n",
    "\n",
    "if not FIREBASE_CREDS:\n",
    "    raise RuntimeError(\"SPARC_FIREBASE_CREDS is empty; set Firebase service account path\")\n",
    "if not os.path.isfile(FIREBASE_CREDS):\n",
    "    raise RuntimeError(\n",
    "        f\"Firebase credentials file not found: {{FIREBASE_CREDS}}. \"\n",
    "        \"Set SPARC_FIREBASE_CREDS to a valid path.\"\n",
    "    )\n",
    "\n",
    "if not firebase_admin._apps:\n",
    "    cred = credentials.Certificate(FIREBASE_CREDS)\n",
    "    firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "logger = logging.getLogger(\"sparc_backend\")\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    presidio_analyzer = AnalyzerEngine()\n",
    "    presidio_anonymizer = AnonymizerEngine()\n",
    "    PRESIDIO_AVAILABLE = True\n",
    "except Exception as presidio_init_error:\n",
    "    presidio_analyzer = None\n",
    "    presidio_anonymizer = None\n",
    "    PRESIDIO_AVAILABLE = False\n",
    "    logger.warning(\"Presidio initialization failed; using fail-closed redaction placeholders: %s\", presidio_init_error)\n",
    "\n",
    "\n",
    "def sanitize_for_storage(text: Optional[str]) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    if not PRESIDIO_AVAILABLE:\n",
    "        return \"[REDACTED]\"\n",
    "    try:\n",
    "        findings = presidio_analyzer.analyze(text=text, language=\"en\")\n",
    "        if not findings:\n",
    "            return text\n",
    "        return presidio_anonymizer.anonymize(text=text, analyzer_results=findings).text\n",
    "    except Exception:\n",
    "        return \"[REDACTED]\"\n",
    "\n",
    "guardrails_engine = None\n",
    "GUARDRAILS_REFUSAL = \"I can only discuss topics related to HPV vaccination and clinical communication training.\"\n",
    "\n",
    "def load_guardrails_runtime() -> None:\n",
    "    global guardrails_engine\n",
    "    try:\n",
    "        rails_config = RailsConfig.from_path(GUARDRAILS_DIR)\n",
    "        guardrails_engine = LLMRails(rails_config)\n",
    "        logger.info(\"Guardrails runtime loaded from %s\", GUARDRAILS_DIR)\n",
    "    except Exception as guardrails_error:\n",
    "        guardrails_engine = None\n",
    "        logger.exception(\"Guardrails initialization failed: %s\", sanitize_for_storage(str(guardrails_error)))\n",
    "\n",
    "async def _run_guardrails(text: str) -> str:\n",
    "    if guardrails_engine is None:\n",
    "        raise RuntimeError(\"Guardrails runtime not initialized\")\n",
    "    messages = [{\"role\": \"user\", \"content\": text}]\n",
    "    if hasattr(guardrails_engine, \"generate_async\"):\n",
    "        result = await guardrails_engine.generate_async(messages=messages)\n",
    "    else:\n",
    "        result = guardrails_engine.generate(messages=messages)\n",
    "    if isinstance(result, dict):\n",
    "        return str(result.get(\"content\", result))\n",
    "    return str(result)\n",
    "\n",
    "async def enforce_guardrails_input(user_text: str) -> Dict[str, Any]:\n",
    "    if not user_text or not user_text.strip():\n",
    "        return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"empty_input\"}\n",
    "    try:\n",
    "        rails_output = await _run_guardrails(user_text)\n",
    "        blocked = GUARDRAILS_REFUSAL.lower() in rails_output.lower()\n",
    "        if blocked:\n",
    "            return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"input_rails_blocked\"}\n",
    "        return {\"allowed\": True, \"text\": user_text, \"reason\": \"input_rails_allowed\"}\n",
    "    except Exception as guardrails_error:\n",
    "        logger.exception(\"Input guardrails failed: %s\", sanitize_for_storage(str(guardrails_error)))\n",
    "        return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"input_rails_error\"}\n",
    "\n",
    "async def enforce_guardrails_output(output_text: str) -> Dict[str, Any]:\n",
    "    if not output_text or not output_text.strip():\n",
    "        return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"empty_output\"}\n",
    "    try:\n",
    "        rails_output = await _run_guardrails(output_text)\n",
    "        blocked = GUARDRAILS_REFUSAL.lower() in rails_output.lower()\n",
    "        if blocked:\n",
    "            return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"output_rails_blocked\"}\n",
    "        return {\"allowed\": True, \"text\": output_text, \"reason\": \"output_rails_allowed\"}\n",
    "    except Exception as guardrails_error:\n",
    "        logger.exception(\"Output guardrails failed: %s\", sanitize_for_storage(str(guardrails_error)))\n",
    "        return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"output_rails_error\"}\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    await load_models()\n",
    "    yield\n",
    "\n",
    "app = FastAPI(title=\"SPARC-P Multi-Agent Backend\", version=\"1.0.0\", lifespan=lifespan)\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=CORS_ALLOWED_ORIGINS,\n",
    "    allow_credentials=CORS_ALLOW_CREDENTIALS,\n",
    "    allow_methods=CORS_ALLOWED_METHODS,\n",
    "    allow_headers=CORS_ALLOWED_HEADERS,\n",
    ")\n",
    "\n",
    "tokenizer = None\n",
    "adapter_model = None\n",
    "ADAPTER_FOR_MODE = {\n",
    "    \"caregiver\": \"caregiver\",\n",
    "    \"coach\": \"coach\",\n",
    "    \"supervisor\": \"supervisor\",\n",
    "}\n",
    "ADAPTER_PATHS = {\n",
    "    \"caregiver\": os.path.join(MODEL_BASE_PATH, \"CaregiverAgent\"),\n",
    "    \"coach\": os.path.join(MODEL_BASE_PATH, \"C-LEAR_CoachAgent\"),\n",
    "    \"supervisor\": os.path.join(MODEL_BASE_PATH, \"SupervisorAgent\"),\n",
    "}\n",
    "riva_auth = None\n",
    "riva_asr_service = None\n",
    "riva_tts_service = None\n",
    "inference_lock = asyncio.Lock()\n",
    "timeout_state_lock = asyncio.Lock()\n",
    "audio_cache_lock = asyncio.Lock()\n",
    "audio_cache_index: Dict[str, Dict[str, Any]] = {}\n",
    "timeout_failures = {\n",
    "    \"primary_inference\": 0,\n",
    "    \"coach_inference\": 0,\n",
    "    \"riva_tts\": 0,\n",
    "}\n",
    "circuit_open_until = {\n",
    "    \"primary_inference\": 0.0,\n",
    "    \"coach_inference\": 0.0,\n",
    "    \"riva_tts\": 0.0,\n",
    "}\n",
    "\n",
    "def generate_tokens_sync(model, **generate_kwargs):\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(**generate_kwargs)\n",
    "\n",
    "def init_riva_clients() -> None:\n",
    "    global riva_auth, riva_asr_service, riva_tts_service\n",
    "    try:\n",
    "        riva_auth = riva.client.Auth(uri=RIVA_SERVER)\n",
    "        riva_asr_service = riva.client.ASRService(riva_auth)\n",
    "        riva_tts_service = riva.client.SpeechSynthesisService(riva_auth)\n",
    "        logger.info(\"Riva clients initialized for reuse at startup\")\n",
    "    except Exception as riva_init_error:\n",
    "        riva_auth = None\n",
    "        riva_asr_service = None\n",
    "        riva_tts_service = None\n",
    "        logger.warning(\"Riva client initialization failed: %s\", sanitize_for_storage(str(riva_init_error)))\n",
    "\n",
    "def synthesize_tts_sync(text: str, voice_name: str = \"English-US.Female-1\") -> bytes:\n",
    "    if riva_tts_service is None:\n",
    "        raise RuntimeError(\"Riva TTS client is not initialized\")\n",
    "    tts_response = riva_tts_service.synthesize(text, voice_name=voice_name)\n",
    "    return tts_response.audio\n",
    "\n",
    "def ensure_audio_cache_dir() -> None:\n",
    "    Path(SPARC_AUDIO_CACHE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "async def prune_expired_audio_cache(now: Optional[float] = None) -> None:\n",
    "    current_ts = now if now is not None else time.time()\n",
    "    expiry_threshold = current_ts - SPARC_AUDIO_URL_TTL_SECONDS\n",
    "    async with audio_cache_lock:\n",
    "        expired_ids = [\n",
    "            audio_id\n",
    "            for audio_id, metadata in audio_cache_index.items()\n",
    "            if metadata.get(\"created_at\", 0.0) < expiry_threshold\n",
    "        ]\n",
    "        for audio_id in expired_ids:\n",
    "            metadata = audio_cache_index.pop(audio_id, None)\n",
    "            if not metadata:\n",
    "                continue\n",
    "            audio_path = metadata.get(\"path\")\n",
    "            if audio_path and os.path.exists(audio_path):\n",
    "                try:\n",
    "                    os.remove(audio_path)\n",
    "                except OSError:\n",
    "                    logger.warning(\"Failed to remove expired audio cache file: %s\", audio_path)\n",
    "\n",
    "async def persist_tts_audio(audio_bytes: bytes) -> Optional[str]:\n",
    "    if not audio_bytes:\n",
    "        return None\n",
    "    if len(audio_bytes) > TTS_MAX_AUDIO_BYTES:\n",
    "        logger.warning(\n",
    "            \"Skipping TTS audio delivery because payload %d bytes exceeds limit %d bytes\",\n",
    "            len(audio_bytes),\n",
    "            TTS_MAX_AUDIO_BYTES,\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    ensure_audio_cache_dir()\n",
    "    await prune_expired_audio_cache()\n",
    "\n",
    "    audio_id = uuid.uuid4().hex\n",
    "    audio_path = os.path.join(SPARC_AUDIO_CACHE_DIR, f\"{audio_id}.wav\")\n",
    "    with open(audio_path, \"wb\") as audio_file:\n",
    "        audio_file.write(audio_bytes)\n",
    "\n",
    "    async with audio_cache_lock:\n",
    "        audio_cache_index[audio_id] = {\"path\": audio_path, \"created_at\": time.time()}\n",
    "\n",
    "    return f\"/v1/audio/{audio_id}\"\n",
    "\n",
    "async def is_circuit_open(operation: str) -> bool:\n",
    "    now = time.monotonic()\n",
    "    async with timeout_state_lock:\n",
    "        return now < circuit_open_until.get(operation, 0.0)\n",
    "\n",
    "async def record_timeout_event(operation: str) -> bool:\n",
    "    now = time.monotonic()\n",
    "    async with timeout_state_lock:\n",
    "        timeout_failures[operation] = timeout_failures.get(operation, 0) + 1\n",
    "        if timeout_failures[operation] >= CIRCUIT_BREAKER_THRESHOLD:\n",
    "            circuit_open_until[operation] = now + CIRCUIT_BREAKER_RESET_SECONDS\n",
    "            timeout_failures[operation] = 0\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "async def record_success_event(operation: str) -> None:\n",
    "    async with timeout_state_lock:\n",
    "        timeout_failures[operation] = 0\n",
    "        circuit_open_until[operation] = 0.0\n",
    "\n",
    "\n",
    "def select_adapter_for_mode(mode: str) -> str:\n",
    "    normalized = (mode or \"caregiver\").strip().lower()\n",
    "    return ADAPTER_FOR_MODE.get(normalized, \"caregiver\")\n",
    "\n",
    "\n",
    "def require_api_key(x_api_key: Optional[str] = Header(default=None, alias=\"X-API-Key\")) -> str:\n",
    "    \"\"\"Defense-in-depth auth guard for in-app API access.\"\"\"\n",
    "    if not API_AUTH_ENABLED:\n",
    "        return \"auth_disabled\"\n",
    "    if not API_KEY:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"API key auth is enabled but SPARC_API_KEY is not configured\",\n",
    "        )\n",
    "    if x_api_key != API_KEY:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Invalid or missing API key\",\n",
    "        )\n",
    "    return x_api_key\n",
    "\n",
    "\n",
    "async def load_models():\n",
    "    global adapter_model, tokenizer\n",
    "    base_model_name = \"gpt-oss-120b\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        ADAPTER_PATHS[\"caregiver\"],\n",
    "        adapter_name=\"caregiver\",\n",
    "    )\n",
    "    adapter_model.load_adapter(ADAPTER_PATHS[\"coach\"], adapter_name=\"coach\")\n",
    "    adapter_model.load_adapter(ADAPTER_PATHS[\"supervisor\"], adapter_name=\"supervisor\")\n",
    "    adapter_model.set_adapter(\"caregiver\")\n",
    "\n",
    "    load_guardrails_runtime()\n",
    "    init_riva_clients()\n",
    "    ensure_audio_cache_dir()\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    session_id: str = Field(..., min_length=1, max_length=128, pattern=r\"^[a-zA-Z0-9_-]+$\")\n",
    "    user_message: str = Field(..., min_length=1, max_length=10000)\n",
    "    audio_data: Optional[str] = Field(default=None, max_length=2_000_000)\n",
    "\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response_text: str\n",
    "    audio_url: Optional[str] = None\n",
    "    emotion: str\n",
    "    animation_cues: Dict[str, str]\n",
    "    coach_feedback: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    riva_ok = riva_auth is not None and riva_asr_service is not None and riva_tts_service is not None\n",
    "\n",
    "    model_ok = tokenizer is not None and adapter_model is not None\n",
    "    status_text = \"healthy\" if model_ok else \"degraded\"\n",
    "    health_payload = {\n",
    "        \"status\": status_text,\n",
    "        \"models_loaded\": model_ok,\n",
    "        \"ready_for_traffic\": model_ok,\n",
    "        \"riva_connected\": riva_ok,\n",
    "        \"api_auth_enabled\": API_AUTH_ENABLED,\n",
    "        \"api_auth_configured\": bool(API_KEY),\n",
    "        \"api_contract_version\": API_CONTRACT_VERSION,\n",
    "        \"guardrails_loaded\": guardrails_engine is not None,\n",
    "        \"riva_client_pool_initialized\": riva_ok,\n",
    "        \"firebase_creds_configured\": bool(FIREBASE_CREDS),\n",
    "    }\n",
    "    http_status = status.HTTP_200_OK if model_ok else status.HTTP_503_SERVICE_UNAVAILABLE\n",
    "    return JSONResponse(status_code=http_status, content=health_payload)\n",
    "\n",
    "\n",
    "@app.get(\"/v1/audio/{audio_id}\")\n",
    "async def get_tts_audio(audio_id: str, _api_key: str = Depends(require_api_key)):\n",
    "    await prune_expired_audio_cache()\n",
    "    async with audio_cache_lock:\n",
    "        metadata = audio_cache_index.get(audio_id)\n",
    "    if not metadata:\n",
    "        raise HTTPException(status_code=404, detail=\"Audio clip not found or expired\")\n",
    "\n",
    "    audio_path = metadata.get(\"path\")\n",
    "    if not audio_path or not os.path.isfile(audio_path):\n",
    "        async with audio_cache_lock:\n",
    "            audio_cache_index.pop(audio_id, None)\n",
    "        raise HTTPException(status_code=404, detail=\"Audio clip not found or expired\")\n",
    "\n",
    "    return FileResponse(audio_path, media_type=\"audio/wav\", filename=f\"{audio_id}.wav\")\n",
    "\n",
    "@app.post(\"/v1/chat\", response_model=ChatResponse)\n",
    "async def process_chat(request: ChatRequest, _api_key: str = Depends(require_api_key)):\n",
    "    try:\n",
    "        if adapter_model is None or tokenizer is None:\n",
    "            raise HTTPException(status_code=503, detail=\"Model adapters are not initialized\")\n",
    "\n",
    "        session_ref = db.collection(\"sessions\").document(request.session_id)\n",
    "        session_state = session_ref.get().to_dict() or {}\n",
    "\n",
    "        input_guard = await enforce_guardrails_input(request.user_message)\n",
    "        if not input_guard[\"allowed\"]:\n",
    "            return ChatResponse(\n",
    "                response_text=input_guard[\"text\"],\n",
    "                emotion=\"neutral\",\n",
    "                animation_cues={\"gesture\": \"idle\"},\n",
    "                coach_feedback={\"safe\": False, \"reason\": input_guard[\"reason\"]}\n",
    "            )\n",
    "\n",
    "        mode = session_state.get(\"mode\", \"caregiver\")\n",
    "        primary_adapter = select_adapter_for_mode(mode)\n",
    "\n",
    "        prompt = f\"[SESSION: {request.session_id}] User: {input_guard['text']}\\\\nAssistant:\"\n",
    "        model_inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        model_inputs = {k: v.to(adapter_model.device) for k, v in model_inputs.items()}\n",
    "\n",
    "        if await is_circuit_open(\"primary_inference\"):\n",
    "            logger.warning(\"Primary inference circuit open; returning degraded fallback response\")\n",
    "            return ChatResponse(\n",
    "                response_text=\"I’m temporarily unable to generate a response right now. Please try again shortly.\",\n",
    "                audio_url=None,\n",
    "                emotion=\"neutral\",\n",
    "                animation_cues={\"gesture\": \"idle\", \"intensity\": \"low\"},\n",
    "                coach_feedback={\"safe\": True, \"reason\": \"inference_circuit_open\", \"summary\": \"Primary model temporarily unavailable.\"}\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            async with inference_lock:\n",
    "                adapter_model.set_adapter(primary_adapter)\n",
    "                output = await asyncio.wait_for(\n",
    "                    asyncio.to_thread(\n",
    "                        generate_tokens_sync,\n",
    "                        adapter_model,\n",
    "                        **model_inputs,\n",
    "                        max_new_tokens=180,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        pad_token_id=tokenizer.eos_token_id,\n",
    "                    ),\n",
    "                    timeout=LLM_TIMEOUT_SECONDS,\n",
    "                )\n",
    "            await record_success_event(\"primary_inference\")\n",
    "        except asyncio.TimeoutError:\n",
    "            circuit_opened = await record_timeout_event(\"primary_inference\")\n",
    "            logger.warning(\"Primary inference timed out after %.1fs%s\", LLM_TIMEOUT_SECONDS, \"; circuit opened\" if circuit_opened else \"\")\n",
    "            return ChatResponse(\n",
    "                response_text=\"I’m temporarily unable to generate a response right now. Please try again shortly.\",\n",
    "                audio_url=None,\n",
    "                emotion=\"neutral\",\n",
    "                animation_cues={\"gesture\": \"idle\", \"intensity\": \"low\"},\n",
    "                coach_feedback={\"safe\": True, \"reason\": \"inference_timeout\", \"summary\": \"Primary model timeout fallback.\"}\n",
    "            )\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response_text = decoded.split(\"Assistant:\")[-1].strip() or \"I’m here to help with HPV vaccine communication practice.\"\n",
    "\n",
    "        output_guard = await enforce_guardrails_output(response_text)\n",
    "        response_text = output_guard[\"text\"]\n",
    "\n",
    "        coach_feedback_text = \"Coach feedback temporarily unavailable.\"\n",
    "        coach_feedback_reason = output_guard[\"reason\"]\n",
    "        try:\n",
    "            if await is_circuit_open(\"coach_inference\"):\n",
    "                logger.warning(\"Coach inference circuit open; skipping coach generation\")\n",
    "                coach_feedback_reason = \"coach_circuit_open\"\n",
    "            else:\n",
    "                feedback_prompt = f\"Provide concise coaching feedback for this response: {response_text}\"\n",
    "                feedback_inputs = tokenizer(feedback_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                feedback_inputs = {k: v.to(adapter_model.device) for k, v in feedback_inputs.items()}\n",
    "                async with inference_lock:\n",
    "                    adapter_model.set_adapter(\"coach\")\n",
    "                    feedback_tokens = await asyncio.wait_for(\n",
    "                        asyncio.to_thread(\n",
    "                            generate_tokens_sync,\n",
    "                            adapter_model,\n",
    "                            **feedback_inputs,\n",
    "                            max_new_tokens=80,\n",
    "                            do_sample=False,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                        ),\n",
    "                        timeout=COACH_TIMEOUT_SECONDS,\n",
    "                    )\n",
    "                await record_success_event(\"coach_inference\")\n",
    "                coach_feedback_text = tokenizer.decode(feedback_tokens[0], skip_special_tokens=True)\n",
    "        except asyncio.TimeoutError:\n",
    "            circuit_opened = await record_timeout_event(\"coach_inference\")\n",
    "            logger.warning(\"Coach inference timed out after %.1fs%s\", COACH_TIMEOUT_SECONDS, \"; circuit opened\" if circuit_opened else \"\")\n",
    "            coach_feedback_reason = \"coach_timeout\"\n",
    "        except Exception as coach_error:\n",
    "            logger.warning(\"Coach inference failed: %s\", sanitize_for_storage(str(coach_error)))\n",
    "            coach_feedback_reason = \"coach_error\"\n",
    "        finally:\n",
    "            async with inference_lock:\n",
    "                adapter_model.set_adapter(primary_adapter)\n",
    "\n",
    "        audio_url = None\n",
    "        try:\n",
    "            if await is_circuit_open(\"riva_tts\"):\n",
    "                logger.warning(\"Riva TTS circuit open; skipping speech synthesis\")\n",
    "            else:\n",
    "                audio_bytes = await asyncio.wait_for(\n",
    "                    asyncio.to_thread(synthesize_tts_sync, response_text, \"English-US.Female-1\"),\n",
    "                    timeout=TTS_TIMEOUT_SECONDS,\n",
    "                )\n",
    "                await record_success_event(\"riva_tts\")\n",
    "                audio_url = await persist_tts_audio(audio_bytes)\n",
    "        except asyncio.TimeoutError:\n",
    "            circuit_opened = await record_timeout_event(\"riva_tts\")\n",
    "            logger.warning(\"Riva TTS timed out after %.1fs%s\", TTS_TIMEOUT_SECONDS, \"; circuit opened\" if circuit_opened else \"\")\n",
    "        except Exception as riva_error:\n",
    "            logger.warning(\"Riva TTS unavailable: %s\", sanitize_for_storage(str(riva_error)))\n",
    "\n",
    "        sanitized_user_message = sanitize_for_storage(request.user_message)\n",
    "        sanitized_response_text = sanitize_for_storage(response_text)\n",
    "        session_state[\"last_user_message\"] = sanitized_user_message\n",
    "        session_state[\"last_response\"] = sanitized_response_text\n",
    "        session_state[\"mode\"] = mode\n",
    "        session_state[\"phi_redaction\"] = \"presidio\"\n",
    "        session_state[\"phi_redaction_applied\"] = True\n",
    "        session_ref.set(session_state, merge=True)\n",
    "\n",
    "        return ChatResponse(\n",
    "            response_text=response_text,\n",
    "            audio_url=audio_url,\n",
    "            emotion=\"supportive\",\n",
    "            animation_cues={\"gesture\": \"speaking\", \"intensity\": \"low\"},\n",
    "            coach_feedback={\"safe\": output_guard[\"allowed\"], \"reason\": coach_feedback_reason, \"summary\": coach_feedback_text[:500]},\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.exception(\"/v1/chat failed after sanitization path: %s\", sanitize_for_storage(str(e)))\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "''').strip()\n",
    "main_content = (\n",
    "    main_content\n",
    "    .replace(\"{MODEL_DIR}\", str(MODEL_DIR))\n",
    "    .replace(\"{PUBAPPS_ROOT}\", str(PUBAPPS_ROOT))\n",
    "    .replace(\"{PUBAPP_ALLOWED_ORIGINS}\", str(PUBAPP_ALLOWED_ORIGINS))\n",
    ")\n",
    "\n",
    "BACKEND_DIR.mkdir(parents=True, exist_ok=True)\n",
    "main_py.write_text(main_content)\n",
    "print(f\"Wrote {main_py}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2bc36",
   "metadata": {},
   "source": [
    "A comprehensive automated check scans the `main.py` file and asserts that over 80 specific code patterns are present (and several dangerous legacy patterns are absent).\n",
    "\n",
    "Think of it as a build-quality gate: before deploying the application, this step verifies that all the critical security, reliability, and compliance features are actually in the code.\n",
    "\n",
    "The checks are grouped into categories:\n",
    "- **Adapter management (C4/C5):** Confirms all three LLM adapters (caregiver, coach, supervisor) are registered by name using `adapter_name=` parameters — not as three separate model objects (which would triple GPU memory usage).\n",
    "- **API authentication (M7):** Verifies the `require_api_key` auth guard is defined and injected via `Depends()` into the chat endpoint.\n",
    "- **Environment config (M8):** Confirms all sensitive values (Firebase path, Riva URL, model path, CORS origins) are read from environment variables — not hard-coded.\n",
    "- **PII redaction (M9/L5):** Verifies Presidio is imported and `sanitize_for_storage()` is called on both the user message and response before Firebase writes.\n",
    "- **CORS security (H3):** Checks that `allow_origins=[\\\"*\\\"]` (wildcard) is absent and specific allowed origins are configured.\n",
    "- **Guardrails (H5):** Confirms NeMo Guardrails is imported and both input and output enforcement functions are called.\n",
    "- **Async inference (H12):** Validates that `asyncio.wait_for()` and `asyncio.to_thread()` are used for model calls — preventing the event loop from blocking during inference.\n",
    "- **Circuit breaker (H13):** Checks that timeout and circuit breaker functions are defined and wired up for all three operations (inference, coach, TTS).\n",
    "- **Quantization (H15):** Confirms 4-bit NF4 quantization config is present — the memory optimization that makes the 120B-parameter model fit on an L4 GPU.\n",
    "\n",
    "> **If any assertion fails:** The error message tells you exactly which marker is missing or which blocked pattern is still present, so you know exactly what needs to be fixed in the main.py before deploying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 C4/C5/M7/M8/M9/M11/L5/H2/H3/H5/H10/H11/H12/H13/H14/H15 Smoke Test — Adapter/Auth/Config + Timeout/Circuit-Breaker + Riva Client Reuse + Bounded TTS Delivery + Lifespan Lifecycle + Redaction + Contract + CORS + Guardrails + Async Inference + Health Readiness + Error Sanitization + Schema Constraints + Quantization\n",
    "\n",
    "backend_text = main_py.read_text()\n",
    "\n",
    "required_markers = [\n",
    "    \"adapter_name=\\\"caregiver\\\"\",\n",
    "    \"load_adapter(ADAPTER_PATHS[\\\"coach\\\"], adapter_name=\\\"coach\\\")\",\n",
    "    \"load_adapter(ADAPTER_PATHS[\\\"supervisor\\\"], adapter_name=\\\"supervisor\\\")\",\n",
    "    \"adapter_model.set_adapter(primary_adapter)\",\n",
    "    \"adapter_model.set_adapter(\\\"coach\\\")\",\n",
    "    \"def require_api_key(\",\n",
    "    \"Header(default=None, alias=\\\"X-API-Key\\\")\",\n",
    "    \"Depends(require_api_key)\",\n",
    "    \"SPARC_FIREBASE_CREDS\",\n",
    "    \"SPARC_MODEL_BASE_PATH\",\n",
    "    \"SPARC_RIVA_SERVER\",\n",
    "    \"os.path.isfile(FIREBASE_CREDS)\",\n",
    "    \"from presidio_analyzer import AnalyzerEngine\",\n",
    "    \"from presidio_anonymizer import AnonymizerEngine\",\n",
    "    \"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\",\n",
    "    \"def sanitize_for_storage(\",\n",
    "    \"sanitized_user_message = sanitize_for_storage(request.user_message)\",\n",
    "    \"sanitized_response_text = sanitize_for_storage(response_text)\",\n",
    "    \"session_state[\\\"phi_redaction_applied\\\"] = True\",\n",
    "    \"API_CONTRACT_VERSION = \\\"v1\\\"\",\n",
    "    \"session_id: str = Field(..., min_length=1, max_length=128, pattern=r\\\"^[a-zA-Z0-9_-]+$\\\")\",\n",
    "    \"user_message: str = Field(..., min_length=1, max_length=10000)\",\n",
    "    \"audio_data: Optional[str] = Field(default=None, max_length=2_000_000)\",\n",
    "    \"api_contract_version\\\": API_CONTRACT_VERSION\",\n",
    "    \"CORS_ALLOWED_ORIGINS = [\",\n",
    "    \"CORS_ALLOW_CREDENTIALS = os.getenv(\\\"SPARC_CORS_ALLOW_CREDENTIALS\\\", \\\"false\\\")\",\n",
    "    \"allow_origins=CORS_ALLOWED_ORIGINS\",\n",
    "    \"allow_credentials=CORS_ALLOW_CREDENTIALS\",\n",
    "    \"from nemoguardrails import LLMRails, RailsConfig\",\n",
    "    \"load_guardrails_runtime()\",\n",
    "    \"enforce_guardrails_input(request.user_message)\",\n",
    "    \"enforce_guardrails_output(response_text)\",\n",
    "    \"guardrails_loaded\\\": guardrails_engine is not None\",\n",
    "    \"import asyncio\",\n",
    "    \"inference_lock = asyncio.Lock()\",\n",
    "    \"LLM_TIMEOUT_SECONDS = float(os.getenv(\\\"SPARC_LLM_TIMEOUT_SECONDS\\\", \\\"10\\\"))\",\n",
    "    \"COACH_TIMEOUT_SECONDS = float(os.getenv(\\\"SPARC_COACH_TIMEOUT_SECONDS\\\", \\\"10\\\"))\",\n",
    "    \"TTS_TIMEOUT_SECONDS = float(os.getenv(\\\"SPARC_TTS_TIMEOUT_SECONDS\\\", \\\"5\\\"))\",\n",
    "    \"TTS_MAX_AUDIO_BYTES = int(os.getenv(\\\"SPARC_TTS_MAX_AUDIO_BYTES\\\", \\\"524288\\\"))\",\n",
    "    \"SPARC_AUDIO_URL_TTL_SECONDS = float(os.getenv(\\\"SPARC_AUDIO_URL_TTL_SECONDS\\\", \\\"300\\\"))\",\n",
    "    \"SPARC_AUDIO_CACHE_DIR = os.getenv(\\\"SPARC_AUDIO_CACHE_DIR\\\", os.path.join(tempfile.gettempdir(), \\\"sparc_tts_audio\\\"))\",\n",
    "    \"from contextlib import asynccontextmanager\",\n",
    "    \"async def lifespan(app: FastAPI):\",\n",
    "    \"await load_models()\",\n",
    "    \"lifespan=lifespan\",\n",
    "    \"CIRCUIT_BREAKER_THRESHOLD = int(os.getenv(\\\"SPARC_TIMEOUT_CIRCUIT_THRESHOLD\\\", \\\"3\\\"))\",\n",
    "    \"CIRCUIT_BREAKER_RESET_SECONDS = float(os.getenv(\\\"SPARC_TIMEOUT_CIRCUIT_RESET_SECONDS\\\", \\\"30\\\"))\",\n",
    "    \"def init_riva_clients() -> None:\",\n",
    "    \"riva_asr_service = riva.client.ASRService(riva_auth)\",\n",
    "    \"riva_tts_service = riva.client.SpeechSynthesisService(riva_auth)\",\n",
    "    \"init_riva_clients()\",\n",
    "    \"if riva_tts_service is None:\",\n",
    "    \"riva_client_pool_initialized\\\": riva_ok\",\n",
    "    \"async def is_circuit_open(operation: str) -> bool:\",\n",
    "    \"async def record_timeout_event(operation: str) -> bool:\",\n",
    "    \"def generate_tokens_sync(\",\n",
    "    \"def synthesize_tts_sync(\",\n",
    "    \"async def persist_tts_audio(audio_bytes: bytes) -> Optional[str]:\",\n",
    "    \"@app.get(\\\"/v1/audio/{audio_id}\\\")\",\n",
    "    \"return FileResponse(audio_path, media_type=\\\"audio/wav\\\", filename=f\\\"{audio_id}.wav\\\")\",\n",
    "    \"asyncio.wait_for(\",\n",
    "    \"await asyncio.to_thread(\",\n",
    "    \"Primary inference timed out after\",\n",
    "    \"Coach inference timed out after\",\n",
    "    \"Riva TTS timed out after\",\n",
    "    \"from fastapi.responses import JSONResponse\",\n",
    "    \"model_ok = tokenizer is not None and adapter_model is not None\",\n",
    "    \"ready_for_traffic\\\": model_ok\",\n",
    "    \"status.HTTP_503_SERVICE_UNAVAILABLE\",\n",
    "    \"return JSONResponse(status_code=http_status, content=health_payload)\",\n",
    "    \"logger.exception(\\\"/v1/chat failed after sanitization path: %s\\\", sanitize_for_storage(str(e)))\",\n",
    "    \"raise HTTPException(status_code=500, detail=\\\"Internal server error\\\")\",\n",
    "    \"bnb_config = BitsAndBytesConfig(\",\n",
    "    \"quantization_config=bnb_config\",\n",
    "    \"bnb_4bit_quant_type=\\\"nf4\\\"\",\n",
    "    \"bnb_4bit_compute_dtype=torch.bfloat16\",\n",
    "]\n",
    "\n",
    "missing = [marker for marker in required_markers if marker not in backend_text]\n",
    "assert not missing, f\"Missing required markers: {missing}\"\n",
    "\n",
    "assert \"caregiver_model = PeftModel.from_pretrained(base_model\" not in backend_text, \"Legacy shared-object adapter pattern remains\"\n",
    "assert \"coach_model = PeftModel.from_pretrained(base_model\" not in backend_text, \"Legacy shared-object adapter pattern remains\"\n",
    "assert \"supervisor_model = PeftModel.from_pretrained(base_model\" not in backend_text, \"Legacy shared-object adapter pattern remains\"\n",
    "assert \"async def process_chat(request: ChatRequest):\" not in backend_text, \"Endpoint still lacks auth dependency\"\n",
    "assert \"session_state[\\\"last_user_message\\\"] = request.user_message\" not in backend_text, \"Raw user message still persisted to Firebase\"\n",
    "assert \"session_state[\\\"last_response\\\"] = response_text\" not in backend_text, \"Raw response still persisted to Firebase\"\n",
    "assert \"user_transcript\" not in backend_text, \"Legacy request field still present\"\n",
    "assert \"allow_origins=[\\\"*\\\"]\" not in backend_text, \"Wildcard CORS origins remain configured\"\n",
    "assert \"allow_credentials=True\" not in backend_text, \"Credentialed wildcard CORS remains configured\"\n",
    "assert \"blocked = [\\\"politics\\\", \\\"election\\\", \\\"gambling\\\", \\\"crypto\\\", \\\"finance advice\\\"]\" not in backend_text, \"Legacy keyword blocklist remains configured\"\n",
    "assert \"output = adapter_model.generate(\" not in backend_text, \"Primary generation still blocks event loop\"\n",
    "assert \"feedback_tokens = adapter_model.generate(\" not in backend_text, \"Coach generation still blocks event loop\"\n",
    "assert \"\\\"models_loaded\\\": True\" not in backend_text, \"Health still hard-codes models_loaded=True\"\n",
    "assert \"detail=str(e)\" not in backend_text, \"Raw exception details still leak to client\"\n",
    "assert \"load_in_4bit=True,\" not in backend_text, \"Legacy direct load_in_4bit kwarg still present\"\n",
    "assert \"data:audio/wav;base64\" not in backend_text, \"Inline base64 audio delivery still present\"\n",
    "assert \"base64.b64encode(\" not in backend_text, \"Inline base64 encoding still present\"\n",
    "assert \"@app.on_event(\\\"startup\\\")\" not in backend_text, \"Deprecated FastAPI startup event hook still present\"\n",
    "\n",
    "print(\"✅ C4/C5/M7/M8/M9/M11/L5/H2/H3/H5/H10/H11/H12/H13/H14/H15 validation passed: named adapters, auth guard, timeout/circuit-breaker policy, startup-initialized reusable Riva clients, bounded TTS URL delivery with payload limits, lifespan-based FastAPI lifecycle initialization, env config, Presidio redaction, unified v1 API contract, safe CORS policy, runtime Guardrails pipeline, non-blocking async inference path, readiness-aware health behavior, sanitized client error responses, strict request schema constraints, and explicit 4-bit quantization config are configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158ffd1",
   "metadata": {},
   "source": [
    "`h11_health_load_test.py` is generated and saved to the backend directory — the load test is designed to be run separately on the PubApps VM against the live service, not from within this workflow.\n",
    "\n",
    "What the load test script does when you run it:\n",
    "- **Fires 30 concurrent chat requests** (`POST /v1/chat`) using a thread pool, simulating 30 simultaneous users sending messages about HPV vaccines to the backend. This stress-tests the async inference pipeline.\n",
    "- **Simultaneously pings `/health` every 200ms for 12 seconds** — for a total of 60 health check calls — to measure how the health endpoint responds *while* the backend is under load from the chat requests.\n",
    "- **Measures p95 latency** for health checks (the 95th percentile, meaning 95% of checks must complete within this time).\n",
    "- **Asserts three conditions:**\n",
    "  1. All 30 chat requests return a recognized status code (200 OKs, 401 if API key is wrong in the test, or 422 for validation errors — but not 500 errors).\n",
    "  2. 99% of health checks must complete successfully within 1.5 seconds.\n",
    "  3. The p95 health latency must be under 1,500ms — confirming the health endpoint stays responsive even when inference is running.\n",
    "\n",
    "> **To run this test:** After the backend is live, SSH to the PubApps VM, go to the backend directory, and run `python h11_health_load_test.py`. Set `SPARC_API_KEY` and `SPARC_BASE_URL` environment variables first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c289c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 H11 Load Test Script — Health Responsiveness Under Chat Load\n",
    "load_test_py = BACKEND_DIR / \"h11_health_load_test.py\"\n",
    "load_test_content = textwrap.dedent(\"\"\"\n",
    "import os\n",
    "import time\n",
    "import statistics\n",
    "import concurrent.futures\n",
    "\n",
    "import requests\n",
    "\n",
    "BASE_URL = os.getenv(\"SPARC_BASE_URL\", \"http://localhost:8000\")\n",
    "API_KEY = os.getenv(\"SPARC_API_KEY\", \"\")\n",
    "HEADERS = {\"X-API-Key\": API_KEY} if API_KEY else {}\n",
    "CHAT_PAYLOAD = {\n",
    "    \"session_id\": \"h11-load\",\n",
    "    \"user_message\": \"Help me discuss HPV vaccines with a hesitant caregiver.\"\n",
    "}\n",
    "\n",
    "\n",
    "def post_chat() -> int:\n",
    "    response = requests.post(f\"{BASE_URL}/v1/chat\", json=CHAT_PAYLOAD, headers=HEADERS, timeout=120)\n",
    "    return response.status_code\n",
    "\n",
    "\n",
    "def ping_health() -> float:\n",
    "    start = time.perf_counter()\n",
    "    response = requests.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    response.raise_for_status()\n",
    "    return (time.perf_counter() - start) * 1000\n",
    "\n",
    "\n",
    "health_latencies = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=12) as pool:\n",
    "    chat_futures = [pool.submit(post_chat) for _ in range(30)]\n",
    "    for _ in range(60):\n",
    "        health_latencies.append(ping_health())\n",
    "        time.sleep(0.2)\n",
    "    chat_statuses = [f.result() for f in chat_futures]\n",
    "\n",
    "health_p95 = statistics.quantiles(health_latencies, n=20)[18] if len(health_latencies) >= 20 else max(health_latencies)\n",
    "health_success_ratio = sum(1 for latency in health_latencies if latency < 1500) / len(health_latencies)\n",
    "\n",
    "assert all(code in (200, 401, 422) for code in chat_statuses), f\"Unexpected chat status codes: {sorted(set(chat_statuses))}\"\n",
    "assert health_success_ratio >= 0.99, f\"Health responsiveness dropped below target: {health_success_ratio:.3f}\"\n",
    "assert health_p95 < 1500, f\"Health p95 latency too high under chat load: {health_p95:.1f}ms\"\n",
    "\n",
    "print(f\"✅ H11 load test passed: /health p95={health_p95:.1f}ms, success_ratio={health_success_ratio:.3f}\")\n",
    "\"\"\").strip()\n",
    "load_test_py.write_text(load_test_content)\n",
    "print(f\"Wrote {load_test_py}\")\n",
    "print(\"Run with: python h11_health_load_test.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b28fc",
   "metadata": {},
   "source": [
    "`h15_quantization_memory_check.py` is generated and saved to the backend directory. Like the load test, it's meant to be run on the live PubApps VM, not from within this workflow.\n",
    "\n",
    "What the script measures and why it matters:\n",
    "\n",
    "The L4 GPU has **24 GB of VRAM** total. The SPARC-P system needs to share this between three components:\n",
    "- The fine-tuned LLM (120B parameters in 4-bit quantization ≈ ~13 GB)\n",
    "- The NVIDIA Riva ASR and TTS models (≈ ~3 GB combined, running in a separate container)\n",
    "- System overhead and CUDA libraries (≈ ~1–2 GB)\n",
    "\n",
    "That leaves only ~7 GB headroom. If memory usage grows beyond the expected budget, the system may start throwing CUDA out-of-memory errors during inference — causing 500 errors for users.\n",
    "\n",
    "What the script does:\n",
    "1. Checks that CUDA is available (fails loudly if not — this script is useless without a GPU).\n",
    "2. Calls `torch.cuda.synchronize()` to ensure all pending CUDA operations are flushed.\n",
    "3. Reads `memory_allocated()` (actively used by tensors), `memory_reserved()` (total pool held by PyTorch), and total `capacity_gb` from the GPU device.\n",
    "4. **Asserts that reserved memory is under 22.0 GB** — leaving at least 2 GB headroom on a 24 GB L4.\n",
    "\n",
    "> **To run:** After the backend has been running for a few minutes (so the model is fully loaded), SSH to the PubApps VM and run `python h15_quantization_memory_check.py` from the backend directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 H15 Quantization Memory Profile Check Script\n",
    "memory_check_py = BACKEND_DIR / \"h15_quantization_memory_check.py\"\n",
    "memory_check_content = textwrap.dedent(\"\"\"\n",
    "import torch\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA device is required for H15 memory profile check\")\n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    allocated_gb = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "    reserved_gb = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "    capacity_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "\n",
    "    print(f\"GPU memory allocated: {allocated_gb:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {reserved_gb:.2f} GB\")\n",
    "    print(f\"GPU capacity: {capacity_gb:.2f} GB\")\n",
    "\n",
    "    assert reserved_gb < 22.0, (\n",
    "        f\"Reserved memory exceeds expected L4 quantized startup budget: {reserved_gb:.2f} GB\"\n",
    "    )\n",
    "    print(\"✅ H15 memory profile check passed: quantized startup is within expected L4 budget.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\").strip()\n",
    "memory_check_py.write_text(memory_check_content)\n",
    "print(f\"Wrote {memory_check_py}\")\n",
    "print(\"Run with: python h15_quantization_memory_check.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d91994",
   "metadata": {},
   "source": [
    "The systemd service file (`sparc-backend.service`) tells the Linux process manager how to run the FastAPI backend as a persistent service — so it starts automatically and restarts itself if it crashes.\n",
    "\n",
    "What the generated service file specifies, and why each setting matters:\n",
    "- **`After=network.target riva-server.service`**: The backend only starts *after* the Riva speech server is running. Without this ordering, the backend could start before Riva is ready and fail to connect to `localhost:50051`.\n",
    "- **`Requires=riva-server.service`**: If Riva stops, systemd also stops the backend. This prevents the backend from running in a degraded state (no speech services) silently.\n",
    "- **`ExecStart={CONDA_ENV}/bin/uvicorn main:app --host 0.0.0.0 --port 8000 --workers 1`**: Uses the *full absolute path* to the uvicorn binary inside the conda environment — not relying on `PATH`. This guarantees the correct Python environment is used even in a non-interactive systemd session. `--workers 1` is intentional for the 2-core, 16 GB PubApps VM.\n",
    "- **`Environment=PATH={CONDA_ENV}/bin:/usr/bin`**: Sets the PATH so child processes spawned by uvicorn also use the conda environment's binaries.\n",
    "- **`Restart=always` + `RestartSec=10`**: If the process crashes (e.g., CUDA out-of-memory during a night of heavy use), systemd waits 10 seconds and restarts it automatically — no manual intervention required.\n",
    "- The file is written to `~/.config/systemd/user/` — the per-user systemd directory that a non-root user can manage without sudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8de462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Create systemd user service for FastAPI\n",
    "systemd_dir = Path.home() / '.config/systemd/user'\n",
    "systemd_dir.mkdir(parents=True, exist_ok=True)\n",
    "service_file = systemd_dir / 'sparc-backend.service'\n",
    "service_content = textwrap.dedent(f\"\"\"\n",
    "[Unit]\n",
    "Description=SPARC-P FastAPI Backend\n",
    "After=network.target riva-server.service\n",
    "Requires=riva-server.service\n",
    "\n",
    "[Service]\n",
    "Type=simple\n",
    "Environment=PATH={CONDA_ENV}/bin:/usr/bin\n",
    "Environment=PYTHONUNBUFFERED=1\n",
    "WorkingDirectory={BACKEND_DIR}\n",
    "ExecStart={CONDA_ENV}/bin/uvicorn main:app --host 0.0.0.0 --port 8000 --workers {UVICORN_WORKERS}\n",
    "Restart=always\n",
    "RestartSec=10\n",
    "\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "\"\"\").strip()\n",
    "service_file.write_text(service_content)\n",
    "print(f\"Wrote {service_file}\")\n",
    "print(service_content)\n",
    "print(f\"Configured uvicorn workers: {UVICORN_WORKERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2ee99e",
   "metadata": {},
   "source": [
    "This is the final service activation step — it registers the FastAPI backend service with systemd and starts it running. These three commands mirror what was done for the Riva service and complete the PubApps deployment.\n",
    "\n",
    "Step by step:\n",
    "1. **`systemctl --user daemon-reload`**: Tells systemd to re-read all service files from disk, picking up the `sparc-backend.service` file just written by the previous cell.\n",
    "2. **`systemctl --user enable --now sparc-backend`**: \n",
    "   - `enable` — registers the service to start automatically whenever you log in to the PubApps VM (persistent across reboots).\n",
    "   - `--now` — starts the service immediately without waiting for the next login.\n",
    "3. **`systemctl --user status sparc-backend`** (`check=False`): Prints the current status. Expected output: `Active: active (running)`. The `check=False` allows the notebook to continue even if the service is still starting.\n",
    "\n",
    "After completing successfully with `EXECUTE = True`:\n",
    "- The FastAPI backend is live at `http://localhost:8000`\n",
    "- The Riva speech server is live at `localhost:50051`\n",
    "- Both services are persistent and will restart automatically on failure\n",
    "- Run `curl -s http://localhost:8000/health` to confirm the backend is healthy and models are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7878243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Enable backend service\n",
    "run(\"systemctl --user daemon-reload\")\n",
    "run(\"systemctl --user enable --now sparc-backend\")\n",
    "run(\"systemctl --user status sparc-backend --no-pager\", check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53760c43",
   "metadata": {},
   "source": [
    "## 7. Validation Checks\n",
    "Set `EXECUTE = True` before running these checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a63254",
   "metadata": {},
   "source": [
    "The final deployment check verifies the entire SPARC-P deployment is operational by running four diagnostic commands against the live PubApps VM. Switch `EXECUTE = True` before running, otherwise all four commands will just print without executing.\n",
    "\n",
    "What each command checks:\n",
    "1. **`curl -s http://localhost:8000/health`**: Makes an HTTP request to the backend's health endpoint. A healthy response looks like `{\"status\": \"healthy\", \"models_loaded\": true, \"riva_connected\": true, \"guardrails_loaded\": true, ...}`. If you see `\"status\": \"degraded\"` or an HTTP error, the backend is not fully initialized — check the service log.\n",
    "2. **`journalctl --user -u riva-server -n 50`**: Shows the last 50 log lines from the Riva speech server service. Look for lines like `Riva server ready` and confirm there are no CUDA errors or model loading failures.\n",
    "3. **`journalctl --user -u sparc-backend -n 50`**: Shows the last 50 log lines from the FastAPI backend service. Look for uvicorn startup messages and confirm that model adapters, guardrails, and Riva clients all initialized successfully.\n",
    "4. **`ls -lh {MODEL_DIR}`**: Lists the model files in the models directory and their sizes. This confirms the model adapters were transferred from HiPerGator successfully. You should see directories for `CaregiverAgent`, `C-LEAR_CoachAgent`, and `SupervisorAgent` — each several GB in size.\n",
    "\n",
    "> **If health returns `\"models_loaded\": false`:** The LLM adapters failed to load. Common causes: the model directory path is wrong, the PEFT adapter files are missing, or the GPU ran out of memory during loading. Check the backend journal for the specific error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89137975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Health and service checks\n",
    "run(\"curl -s http://localhost:8000/health\", check=False)\n",
    "run(\"journalctl --user -u riva-server -n 50 --no-pager\", check=False)\n",
    "run(\"journalctl --user -u sparc-backend -n 50 --no-pager\", check=False)\n",
    "run(f\"ls -lh {MODEL_DIR}\", check=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
