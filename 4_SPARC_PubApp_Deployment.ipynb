{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd3e648",
   "metadata": {},
   "source": [
    "# SPARC-P PubApps Deployment Notebook\n",
    "\n",
    "This notebook is the runnable version of Step 4 for deploying SPARC-P to UF RC PubApps.\n",
    "\n",
    "## Resource Profiles\n",
    "- **HiPerGator (parallel jobs)**: 4 GPUs, 16 CPU cores\n",
    "- **PubApps (serving)**: 1x L4 GPU (24GB), 2 CPU cores, 16GB RAM\n",
    "\n",
    "## Before You Run\n",
    "- You are on your PubApps VM via SSH\n",
    "- You have your project account (`SPARCP`)\n",
    "- Trained models are available from HiPerGator at `/blue/jasondeanarnold/SPARCP/trained_models`\n",
    "- Podman + systemd user services are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ce951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "import os\n",
    "import subprocess\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT = os.environ.get(\"SPARC_PUBAPPS_PROJECT\", \"SPARCP\")\n",
    "PUBAPPS_ROOT = Path(f\"/pubapps/{PROJECT}\")\n",
    "MODEL_DIR = PUBAPPS_ROOT / \"models\"\n",
    "CONDA_ENV = PUBAPPS_ROOT / \"conda_envs\" / \"sparc_backend\"\n",
    "BACKEND_DIR = PUBAPPS_ROOT / \"backend\"\n",
    "RIVA_MODEL_DIR = PUBAPPS_ROOT / \"riva_models\"\n",
    "\n",
    "HIPERGATOR_SOURCE_MODELS = \"/blue/jasondeanarnold/SPARCP/trained_models\"\n",
    "PUBAPPS_HOST = os.environ.get(\"SPARC_PUBAPPS_HOST\", \"pubapps-vm.rc.ufl.edu\")\n",
    "\n",
    "# Resource constraints\n",
    "HPG_MAX_GPUS = 4\n",
    "HPG_MAX_CORES = 16\n",
    "PUBAPPS_GPU = \"L4 (24GB)\"\n",
    "PUBAPPS_CORES = 2\n",
    "PUBAPPS_RAM_GB = 16\n",
    "UVICORN_WORKERS = 1  # tuned for 2 CPU cores and 16GB RAM\n",
    "\n",
    "print(f\"Project: {PROJECT}\")\n",
    "print(f\"PubApps root: {PUBAPPS_ROOT}\")\n",
    "print(f\"Conda env: {CONDA_ENV}\")\n",
    "print(f\"Backend dir: {BACKEND_DIR}\")\n",
    "print(f\"HiPerGator resources: {HPG_MAX_GPUS} GPUs, {HPG_MAX_CORES} cores\")\n",
    "print(f\"PubApps resources: {PUBAPPS_GPU}, {PUBAPPS_CORES} cores, {PUBAPPS_RAM_GB}GB RAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Command runner (safe by default)\n",
    "EXECUTE = False  # Set True to actually run shell commands\n",
    "\n",
    "def run(cmd: str, check: bool = True):\n",
    "    print(f\"$ {cmd}\")\n",
    "    if not EXECUTE:\n",
    "        print(\"(dry-run) command not executed\\n\")\n",
    "        return None\n",
    "    result = subprocess.run([\"bash\", \"-lc\", cmd], capture_output=True, text=True)\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(result.stderr)\n",
    "    if check and result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
    "    print()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6cdeb",
   "metadata": {},
   "source": [
    "## 3. Transfer Models from HiPerGator\n",
    "Run this on HiPerGator or from a hop host with access to both systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Render model sync command\n",
    "rsync_cmd = textwrap.dedent(f\"\"\"\n",
    "rsync -avz --progress \\\n",
    "  {HIPERGATOR_SOURCE_MODELS}/ \\\n",
    "  {PROJECT}@{PUBAPPS_HOST}:{MODEL_DIR}/\n",
    "\"\"\").strip()\n",
    "print(rsync_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f365d",
   "metadata": {},
   "source": [
    "## 4. PubApps Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89578cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Create required directories\n",
    "run(f\"mkdir -p {PUBAPPS_ROOT} {MODEL_DIR} {BACKEND_DIR} {RIVA_MODEL_DIR} {PUBAPPS_ROOT / 'conda_envs'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Create backend conda environment\n",
    "run(\"conda --version\", check=False)\n",
    "run(f\"cd {PUBAPPS_ROOT}; conda env create -f environment_backend.yml -p {CONDA_ENV}\")\n",
    "run(f\"conda run -p {CONDA_ENV} python -c 'import fastapi,langgraph,torch; print(\\\"backend env ok\\\")'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53132751",
   "metadata": {},
   "source": [
    "## 5. Deploy Riva with Podman + Quadlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80caf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Write quadlet service for Riva\n",
    "quadlet_dir = Path.home() / '.config/containers/systemd'\n",
    "quadlet_dir.mkdir(parents=True, exist_ok=True)\n",
    "quadlet_file = quadlet_dir / 'riva-server.container'\n",
    "quadlet_content = textwrap.dedent(f\"\"\"\n",
    "[Unit]\n",
    "Description=SPARC-P Riva Speech Server\n",
    "After=network-online.target\n",
    "\n",
    "[Container]\n",
    "Image=nvcr.io/nvidia/riva/riva-speech:2.16.0-server\n",
    "ContainerName=riva-server\n",
    "AddDevice=/dev/nvidia0\n",
    "AddDevice=/dev/nvidiactl\n",
    "AddDevice=/dev/nvidia-uvm\n",
    "Volume={RIVA_MODEL_DIR}:/data:Z\n",
    "PublishPort=50051:50051\n",
    "Environment=NVIDIA_VISIBLE_DEVICES=all\n",
    "Exec=/opt/riva/bin/riva_server --riva_model_repo=/data/models\n",
    "\n",
    "[Service]\n",
    "Restart=always\n",
    "TimeoutStartSec=300\n",
    "\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "\"\"\").strip()\n",
    "quadlet_file.write_text(quadlet_content)\n",
    "print(f\"Wrote {quadlet_file}\")\n",
    "print(quadlet_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73679112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Pull image and enable Riva service\n",
    "run(\"podman pull nvcr.io/nvidia/riva/riva-speech:2.16.0-server\")\n",
    "run(\"systemctl --user daemon-reload\")\n",
    "run(\"systemctl --user enable --now riva-server\")\n",
    "run(\"systemctl --user status riva-server --no-pager\", check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454afcc",
   "metadata": {},
   "source": [
    "## 6. Create FastAPI Backend + Systemd Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22080f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Write backend main.py (integration-ready)\n",
    "main_py = BACKEND_DIR / 'main.py'\n",
    "main_content = textwrap.dedent(f\"\"\"\n",
    "import base64\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import riva.client\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "\n",
    "MODEL_BASE_PATH = \"{MODEL_DIR}\"\n",
    "RIVA_SERVER = \"localhost:50051\"\n",
    "FIREBASE_CREDS = \"{PUBAPPS_ROOT}/config/firebase-credentials.json\"\n",
    "\n",
    "if not firebase_admin._apps:\n",
    "    cred = credentials.Certificate(FIREBASE_CREDS)\n",
    "    firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "app = FastAPI(title=\"SPARC-P Multi-Agent Backend\", version=\"1.0.0\")\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "caregiver_model = coach_model = supervisor_model = tokenizer = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_models():\n",
    "    global caregiver_model, coach_model, supervisor_model, tokenizer\n",
    "    base_model_name = \"gpt-oss-120b\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, load_in_4bit=True, device_map=\"auto\")\n",
    "    caregiver_model = PeftModel.from_pretrained(base_model, os.path.join(MODEL_BASE_PATH, \"CaregiverAgent\"))\n",
    "    coach_model = PeftModel.from_pretrained(base_model, os.path.join(MODEL_BASE_PATH, \"C-LEAR_CoachAgent\"))\n",
    "    supervisor_model = PeftModel.from_pretrained(base_model, os.path.join(MODEL_BASE_PATH, \"SupervisorAgent\"))\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    session_id: str\n",
    "    user_message: str\n",
    "    audio_data: Optional[str] = None\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response_text: str\n",
    "    audio_url: Optional[str] = None\n",
    "    emotion: str\n",
    "    animation_cues: dict\n",
    "    coach_feedback: Optional[dict] = None\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    try:\n",
    "        auth = riva.client.Auth(uri=RIVA_SERVER)\n",
    "        riva.client.ASRService(auth)\n",
    "        riva_ok = True\n",
    "    except Exception:\n",
    "        riva_ok = False\n",
    "    return {\"status\": \"healthy\", \"riva_connected\": riva_ok}\n",
    "\n",
    "@app.post(\"/v1/chat\", response_model=ChatResponse)\n",
    "async def process_chat(request: ChatRequest):\n",
    "    try:\n",
    "        session_ref = db.collection(\"sessions\").document(request.session_id)\n",
    "        session_state = session_ref.get().to_dict() or {}\n",
    "\n",
    "        blocked = [\"politics\", \"election\", \"gambling\", \"crypto\", \"finance advice\"]\n",
    "        lower = request.user_message.lower()\n",
    "        if any(t in lower for t in blocked):\n",
    "            return ChatResponse(\n",
    "                response_text=\"I can only discuss HPV vaccination and clinical communication practice.\",\n",
    "                emotion=\"neutral\",\n",
    "                animation_cues={\"gesture\": \"idle\"},\n",
    "                coach_feedback={\"safe\": False, \"reason\": \"off_topic\"}\n",
    "            )\n",
    "\n",
    "        mode = session_state.get(\"mode\", \"caregiver\")\n",
    "        active = coach_model if mode == \"coach\" else supervisor_model if mode == \"supervisor\" else caregiver_model\n",
    "\n",
    "        prompt = f\"[SESSION: {request.session_id}] User: {request.user_message}\\nAssistant:\"\n",
    "        model_inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        model_inputs = {k: v.to(active.device) for k, v in model_inputs.items()}\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            output = active.generate(**model_inputs, max_new_tokens=180, do_sample=True, temperature=0.7, top_p=0.9, pad_token_id=tokenizer.eos_token_id)\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response_text = decoded.split(\"Assistant:\")[-1].strip() or \"Iâ€™m here to help with HPV vaccine communication practice.\"\n",
    "\n",
    "        audio_url = None\n",
    "        try:\n",
    "            auth = riva.client.Auth(uri=RIVA_SERVER)\n",
    "            tts = riva.client.SpeechSynthesisService(auth)\n",
    "            tts_resp = tts.synthesize(response_text, voice_name=\"English-US.Female-1\")\n",
    "            audio_url = \"data:audio/wav;base64,\" + base64.b64encode(tts_resp.audio).decode(\"utf-8\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        session_state[\"last_user_message\"] = request.user_message\n",
    "        session_state[\"last_response\"] = response_text\n",
    "        session_state[\"mode\"] = mode\n",
    "        session_ref.set(session_state, merge=True)\n",
    "\n",
    "        return ChatResponse(\n",
    "            response_text=response_text,\n",
    "            audio_url=audio_url,\n",
    "            emotion=\"supportive\",\n",
    "            animation_cues={\"gesture\": \"speaking\", \"intensity\": \"low\"},\n",
    "            coach_feedback={\"safe\": True}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\"\"\").strip()\n",
    "\n",
    "BACKEND_DIR.mkdir(parents=True, exist_ok=True)\n",
    "main_py.write_text(main_content)\n",
    "print(f\"Wrote {main_py}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8de462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Create systemd user service for FastAPI\n",
    "systemd_dir = Path.home() / '.config/systemd/user'\n",
    "systemd_dir.mkdir(parents=True, exist_ok=True)\n",
    "service_file = systemd_dir / 'sparc-backend.service'\n",
    "service_content = textwrap.dedent(f\"\"\"\n",
    "[Unit]\n",
    "Description=SPARC-P FastAPI Backend\n",
    "After=network.target riva-server.service\n",
    "Requires=riva-server.service\n",
    "\n",
    "[Service]\n",
    "Type=simple\n",
    "Environment=PATH={CONDA_ENV}/bin:/usr/bin\n",
    "Environment=PYTHONUNBUFFERED=1\n",
    "WorkingDirectory={BACKEND_DIR}\n",
    "ExecStart={CONDA_ENV}/bin/uvicorn main:app --host 0.0.0.0 --port 8000 --workers {UVICORN_WORKERS}\n",
    "Restart=always\n",
    "RestartSec=10\n",
    "\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "\"\"\").strip()\n",
    "service_file.write_text(service_content)\n",
    "print(f\"Wrote {service_file}\")\n",
    "print(service_content)\n",
    "print(f\"Configured uvicorn workers: {UVICORN_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7878243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Enable backend service\n",
    "run(\"systemctl --user daemon-reload\")\n",
    "run(\"systemctl --user enable --now sparc-backend\")\n",
    "run(\"systemctl --user status sparc-backend --no-pager\", check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53760c43",
   "metadata": {},
   "source": [
    "## 7. Validation Checks\n",
    "Set `EXECUTE = True` before running these checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89137975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Health and service checks\n",
    "run(\"curl -s http://localhost:8000/health\", check=False)\n",
    "run(\"journalctl --user -u riva-server -n 50 --no-pager\", check=False)\n",
    "run(\"journalctl --user -u sparc-backend -n 50 --no-pager\", check=False)\n",
    "run(f\"ls -lh {MODEL_DIR}\", check=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
