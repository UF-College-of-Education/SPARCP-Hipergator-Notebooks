{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd3e648",
   "metadata": {},
   "source": [
    "# SPARC-P PubApps Deployment Notebook\n",
    "\n",
    "This notebook is the runnable version of Step 4 for deploying SPARC-P to UF RC PubApps.\n",
    "\n",
    "## Resource Profiles\n",
    "- **HiPerGator (parallel jobs)**: 4 GPUs, 16 CPU cores\n",
    "- **PubApps (serving)**: 1x L4 GPU (24GB), 2 CPU cores, 16GB RAM\n",
    "\n",
    "## Before You Run\n",
    "- You are on your PubApps VM via SSH\n",
    "- You have your project account (`SPARCP`)\n",
    "- Trained models are available from HiPerGator at `/blue/jasondeanarnold/SPARCP/trained_models`\n",
    "- Podman + systemd user services are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ce951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "import os\n",
    "import subprocess\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT = os.environ.get(\"SPARC_PUBAPPS_PROJECT\", \"SPARCP\")\n",
    "PUBAPPS_ROOT = Path(f\"/pubapps/{PROJECT}\")\n",
    "MODEL_DIR = PUBAPPS_ROOT / \"models\"\n",
    "CONDA_ENV = PUBAPPS_ROOT / \"conda_envs\" / \"sparc_backend\"\n",
    "BACKEND_DIR = PUBAPPS_ROOT / \"backend\"\n",
    "RIVA_MODEL_DIR = PUBAPPS_ROOT / \"riva_models\"\n",
    "\n",
    "BASE_PATH = os.environ.get(\"SPARC_BASE_PATH\", \"/blue/jasondeanarnold/SPARCP\")\n",
    "HIPERGATOR_SOURCE_MODELS = os.environ.get(\n",
    "    \"SPARC_HIPERGATOR_SOURCE_MODELS\",\n",
    "    f\"{BASE_PATH}/trained_models\",\n",
    ")\n",
    "PUBAPPS_HOST = os.environ.get(\"SPARC_PUBAPPS_HOST\", \"pubapps-vm.rc.ufl.edu\")\n",
    "PUBAPPS_SSH_USER = os.environ.get(\"SPARC_PUBAPPS_SSH_USER\", PROJECT)\n",
    "PUBAPP_ALLOWED_ORIGINS = os.environ.get(\n",
    "    \"SPARC_CORS_ALLOWED_ORIGINS\",\n",
    "    \"https://hpvcommunicationtraining.com,https://hpvcommunicationtraining.org\",\n",
    ")\n",
    "FIREBASE_CREDS_PATH = Path(\n",
    "    os.environ.get(\"SPARC_FIREBASE_CREDS\", str(PUBAPPS_ROOT / \"config\" / \"firebase-credentials.json\"))\n",
    ")\n",
    "\n",
    "# Resource constraints\n",
    "HPG_MAX_GPUS = 4\n",
    "HPG_MAX_CORES = 16\n",
    "PUBAPPS_GPU = \"L4 (24GB)\"\n",
    "PUBAPPS_CORES = 2\n",
    "PUBAPPS_RAM_GB = 16\n",
    "UVICORN_WORKERS = 1  # tuned for 2 CPU cores and 16GB RAM\n",
    "\n",
    "print(f\"Project: {PROJECT}\")\n",
    "print(f\"PubApps root: {PUBAPPS_ROOT}\")\n",
    "print(f\"Conda env: {CONDA_ENV}\")\n",
    "print(f\"Backend dir: {BACKEND_DIR}\")\n",
    "print(f\"HiPerGator source models: {HIPERGATOR_SOURCE_MODELS}\")\n",
    "print(f\"PubApps host: {PUBAPPS_HOST}\")\n",
    "print(f\"PubApps SSH user: {PUBAPPS_SSH_USER}\")\n",
    "print(f\"Allowed CORS origins: {PUBAPP_ALLOWED_ORIGINS}\")\n",
    "print(f\"Firebase creds path: {FIREBASE_CREDS_PATH}\")\n",
    "print(f\"HiPerGator resources: {HPG_MAX_GPUS} GPUs, {HPG_MAX_CORES} cores\")\n",
    "print(f\"PubApps resources: {PUBAPPS_GPU}, {PUBAPPS_CORES} cores, {PUBAPPS_RAM_GB}GB RAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Command runner (safe by default)\n",
    "EXECUTE = False  # Set True to actually run shell commands\n",
    "\n",
    "def run(cmd: str, check: bool = True):\n",
    "    print(f\"$ {cmd}\")\n",
    "    if not EXECUTE:\n",
    "        print(\"(dry-run) command not executed\\n\")\n",
    "        return None\n",
    "    result = subprocess.run([\"bash\", \"-lc\", cmd], capture_output=True, text=True)\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(result.stderr)\n",
    "    if check and result.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
    "    print()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6cdeb",
   "metadata": {},
   "source": [
    "## 3. Transfer Models from HiPerGator\n",
    "Run this on HiPerGator or from a hop host with access to both systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Render model sync command\n",
    "rsync_cmd = textwrap.dedent(f\"\"\"\n",
    "rsync -avz --progress \\\n",
    "  {HIPERGATOR_SOURCE_MODELS}/ \\\n",
    "  {PUBAPPS_SSH_USER}@{PUBAPPS_HOST}:{MODEL_DIR}/\n",
    "\"\"\").strip()\n",
    "print(rsync_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f365d",
   "metadata": {},
   "source": [
    "## 4. PubApps Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89578cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Create required directories\n",
    "run(f\"mkdir -p {PUBAPPS_ROOT} {MODEL_DIR} {BACKEND_DIR} {RIVA_MODEL_DIR} {PUBAPPS_ROOT / 'conda_envs'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Create backend conda environment\n",
    "run(\"conda --version\", check=False)\n",
    "run(f\"cd {PUBAPPS_ROOT}; conda env create -f environment_backend.yml -p {CONDA_ENV}\")\n",
    "run(f\"conda run -p {CONDA_ENV} python -c 'import fastapi,langgraph,torch; print(\\\"backend env ok\\\")'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53132751",
   "metadata": {},
   "source": [
    "## 5. Deploy Riva with Podman + Quadlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80caf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Write quadlet service for Riva\n",
    "quadlet_dir = Path.home() / '.config/containers/systemd'\n",
    "quadlet_dir.mkdir(parents=True, exist_ok=True)\n",
    "quadlet_file = quadlet_dir / 'riva-server.container'\n",
    "quadlet_content = textwrap.dedent(f\"\"\"\n",
    "[Unit]\n",
    "Description=SPARC-P Riva Speech Server\n",
    "After=network-online.target\n",
    "\n",
    "[Container]\n",
    "Image=nvcr.io/nvidia/riva/riva-speech:2.16.0-server\n",
    "ContainerName=riva-server\n",
    "AddDevice=/dev/nvidia0\n",
    "AddDevice=/dev/nvidiactl\n",
    "AddDevice=/dev/nvidia-uvm\n",
    "Volume={RIVA_MODEL_DIR}:/data:Z\n",
    "PublishPort=50051:50051\n",
    "Environment=NVIDIA_VISIBLE_DEVICES=all\n",
    "Exec=/opt/riva/bin/riva_server --riva_model_repo=/data/models\n",
    "\n",
    "[Service]\n",
    "Restart=always\n",
    "TimeoutStartSec=300\n",
    "\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "\"\"\").strip()\n",
    "quadlet_file.write_text(quadlet_content)\n",
    "print(f\"Wrote {quadlet_file}\")\n",
    "print(quadlet_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73679112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Pull image and enable Riva service\n",
    "run(\"podman pull nvcr.io/nvidia/riva/riva-speech:2.16.0-server\")\n",
    "run(\"systemctl --user daemon-reload\")\n",
    "run(\"systemctl --user enable --now riva-server\")\n",
    "run(\"systemctl --user status riva-server --no-pager\", check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454afcc",
   "metadata": {},
   "source": [
    "## 6. Create FastAPI Backend + Systemd Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22080f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Write backend main.py (integration-ready)\n",
    "main_py = BACKEND_DIR / 'main.py'\n",
    "main_content = textwrap.dedent('''\n",
    "import asyncio\n",
    "import base64\n",
    "import os\n",
    "import logging\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "from fastapi import Depends, FastAPI, Header, HTTPException, status\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel, Field\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import riva.client\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, firestore\n",
    "\n",
    "MODEL_BASE_PATH = os.getenv(\"SPARC_MODEL_BASE_PATH\", \"{MODEL_DIR}\")\n",
    "RIVA_SERVER = os.getenv(\"SPARC_RIVA_SERVER\", \"localhost:50051\")\n",
    "FIREBASE_CREDS = os.getenv(\"SPARC_FIREBASE_CREDS\", \"{PUBAPPS_ROOT}/config/firebase-credentials.json\")\n",
    "GUARDRAILS_DIR = os.getenv(\"SPARC_GUARDRAILS_DIR\", os.path.join(os.path.dirname(__file__), \"guardrails\"))\n",
    "\n",
    "API_AUTH_ENABLED = os.getenv(\"SPARC_API_AUTH_ENABLED\", \"true\").strip().lower() == \"true\"\n",
    "API_KEY = os.getenv(\"SPARC_API_KEY\", \"\")\n",
    "CORS_ALLOWED_ORIGINS = [\n",
    "    origin.strip()\n",
    "    for origin in os.getenv(\"SPARC_CORS_ALLOWED_ORIGINS\", \"{PUBAPP_ALLOWED_ORIGINS}\").split(\",\")\n",
    "    if origin.strip()\n",
    "]\n",
    "CORS_ALLOW_CREDENTIALS = os.getenv(\"SPARC_CORS_ALLOW_CREDENTIALS\", \"false\").strip().lower() == \"true\"\n",
    "CORS_ALLOWED_METHODS = [\"GET\", \"POST\", \"OPTIONS\"]\n",
    "CORS_ALLOWED_HEADERS = [\"Content-Type\", \"X-API-Key\", \"Authorization\"]\n",
    "API_CONTRACT_VERSION = \"v1\"\n",
    "\n",
    "if not FIREBASE_CREDS:\n",
    "    raise RuntimeError(\"SPARC_FIREBASE_CREDS is empty; set Firebase service account path\")\n",
    "if not os.path.isfile(FIREBASE_CREDS):\n",
    "    raise RuntimeError(\n",
    "        f\"Firebase credentials file not found: {{FIREBASE_CREDS}}. \"\n",
    "        \"Set SPARC_FIREBASE_CREDS to a valid path.\"\n",
    "    )\n",
    "\n",
    "if not firebase_admin._apps:\n",
    "    cred = credentials.Certificate(FIREBASE_CREDS)\n",
    "    firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()\n",
    "\n",
    "logger = logging.getLogger(\"sparc_backend\")\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "try:\n",
    "    presidio_analyzer = AnalyzerEngine()\n",
    "    presidio_anonymizer = AnonymizerEngine()\n",
    "    PRESIDIO_AVAILABLE = True\n",
    "except Exception as presidio_init_error:\n",
    "    presidio_analyzer = None\n",
    "    presidio_anonymizer = None\n",
    "    PRESIDIO_AVAILABLE = False\n",
    "    logger.warning(\"Presidio initialization failed; using fail-closed redaction placeholders: %s\", presidio_init_error)\n",
    "\n",
    "\n",
    "def sanitize_for_storage(text: Optional[str]) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    if not PRESIDIO_AVAILABLE:\n",
    "        return \"[REDACTED]\"\n",
    "    try:\n",
    "        findings = presidio_analyzer.analyze(text=text, language=\"en\")\n",
    "        if not findings:\n",
    "            return text\n",
    "        return presidio_anonymizer.anonymize(text=text, analyzer_results=findings).text\n",
    "    except Exception:\n",
    "        return \"[REDACTED]\"\n",
    "\n",
    "guardrails_engine = None\n",
    "GUARDRAILS_REFUSAL = \"I can only discuss topics related to HPV vaccination and clinical communication training.\"\n",
    "\n",
    "def load_guardrails_runtime() -> None:\n",
    "    global guardrails_engine\n",
    "    try:\n",
    "        rails_config = RailsConfig.from_path(GUARDRAILS_DIR)\n",
    "        guardrails_engine = LLMRails(rails_config)\n",
    "        logger.info(\"Guardrails runtime loaded from %s\", GUARDRAILS_DIR)\n",
    "    except Exception as guardrails_error:\n",
    "        guardrails_engine = None\n",
    "        logger.exception(\"Guardrails initialization failed: %s\", sanitize_for_storage(str(guardrails_error)))\n",
    "\n",
    "async def _run_guardrails(text: str) -> str:\n",
    "    if guardrails_engine is None:\n",
    "        raise RuntimeError(\"Guardrails runtime not initialized\")\n",
    "    messages = [{\"role\": \"user\", \"content\": text}]\n",
    "    if hasattr(guardrails_engine, \"generate_async\"):\n",
    "        result = await guardrails_engine.generate_async(messages=messages)\n",
    "    else:\n",
    "        result = guardrails_engine.generate(messages=messages)\n",
    "    if isinstance(result, dict):\n",
    "        return str(result.get(\"content\", result))\n",
    "    return str(result)\n",
    "\n",
    "async def enforce_guardrails_input(user_text: str) -> Dict[str, Any]:\n",
    "    if not user_text or not user_text.strip():\n",
    "        return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"empty_input\"}\n",
    "    try:\n",
    "        rails_output = await _run_guardrails(user_text)\n",
    "        blocked = GUARDRAILS_REFUSAL.lower() in rails_output.lower()\n",
    "        if blocked:\n",
    "            return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"input_rails_blocked\"}\n",
    "        return {\"allowed\": True, \"text\": user_text, \"reason\": \"input_rails_allowed\"}\n",
    "    except Exception as guardrails_error:\n",
    "        logger.exception(\"Input guardrails failed: %s\", sanitize_for_storage(str(guardrails_error)))\n",
    "        return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"input_rails_error\"}\n",
    "\n",
    "async def enforce_guardrails_output(output_text: str) -> Dict[str, Any]:\n",
    "    if not output_text or not output_text.strip():\n",
    "        return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"empty_output\"}\n",
    "    try:\n",
    "        rails_output = await _run_guardrails(output_text)\n",
    "        blocked = GUARDRAILS_REFUSAL.lower() in rails_output.lower()\n",
    "        if blocked:\n",
    "            return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"output_rails_blocked\"}\n",
    "        return {\"allowed\": True, \"text\": output_text, \"reason\": \"output_rails_allowed\"}\n",
    "    except Exception as guardrails_error:\n",
    "        logger.exception(\"Output guardrails failed: %s\", sanitize_for_storage(str(guardrails_error)))\n",
    "        return {\"allowed\": False, \"text\": GUARDRAILS_REFUSAL, \"reason\": \"output_rails_error\"}\n",
    "\n",
    "app = FastAPI(title=\"SPARC-P Multi-Agent Backend\", version=\"1.0.0\")\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=CORS_ALLOWED_ORIGINS,\n",
    "    allow_credentials=CORS_ALLOW_CREDENTIALS,\n",
    "    allow_methods=CORS_ALLOWED_METHODS,\n",
    "    allow_headers=CORS_ALLOWED_HEADERS,\n",
    ")\n",
    "\n",
    "tokenizer = None\n",
    "adapter_model = None\n",
    "ADAPTER_FOR_MODE = {\n",
    "    \"caregiver\": \"caregiver\",\n",
    "    \"coach\": \"coach\",\n",
    "    \"supervisor\": \"supervisor\",\n",
    "}\n",
    "ADAPTER_PATHS = {\n",
    "    \"caregiver\": os.path.join(MODEL_BASE_PATH, \"CaregiverAgent\"),\n",
    "    \"coach\": os.path.join(MODEL_BASE_PATH, \"C-LEAR_CoachAgent\"),\n",
    "    \"supervisor\": os.path.join(MODEL_BASE_PATH, \"SupervisorAgent\"),\n",
    "}\n",
    "inference_lock = asyncio.Lock()\n",
    "\n",
    "def generate_tokens_sync(model, **generate_kwargs):\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(**generate_kwargs)\n",
    "\n",
    "\n",
    "def select_adapter_for_mode(mode: str) -> str:\n",
    "    normalized = (mode or \"caregiver\").strip().lower()\n",
    "    return ADAPTER_FOR_MODE.get(normalized, \"caregiver\")\n",
    "\n",
    "\n",
    "def require_api_key(x_api_key: Optional[str] = Header(default=None, alias=\"X-API-Key\")) -> str:\n",
    "    \"\"\"Defense-in-depth auth guard for in-app API access.\"\"\"\n",
    "    if not API_AUTH_ENABLED:\n",
    "        return \"auth_disabled\"\n",
    "    if not API_KEY:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,\n",
    "            detail=\"API key auth is enabled but SPARC_API_KEY is not configured\",\n",
    "        )\n",
    "    if x_api_key != API_KEY:\n",
    "        raise HTTPException(\n",
    "            status_code=status.HTTP_401_UNAUTHORIZED,\n",
    "            detail=\"Invalid or missing API key\",\n",
    "        )\n",
    "    return x_api_key\n",
    "\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def load_models():\n",
    "    global adapter_model, tokenizer\n",
    "    base_model_name = \"gpt-oss-120b\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    adapter_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        ADAPTER_PATHS[\"caregiver\"],\n",
    "        adapter_name=\"caregiver\",\n",
    "    )\n",
    "    adapter_model.load_adapter(ADAPTER_PATHS[\"coach\"], adapter_name=\"coach\")\n",
    "    adapter_model.load_adapter(ADAPTER_PATHS[\"supervisor\"], adapter_name=\"supervisor\")\n",
    "    adapter_model.set_adapter(\"caregiver\")\n",
    "\n",
    "    load_guardrails_runtime()\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    session_id: str = Field(..., min_length=1, max_length=128, pattern=r\"^[a-zA-Z0-9_-]+$\")\n",
    "    user_message: str = Field(..., min_length=1, max_length=10000)\n",
    "    audio_data: Optional[str] = Field(default=None, max_length=2_000_000)\n",
    "\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response_text: str\n",
    "    audio_url: Optional[str] = None\n",
    "    emotion: str\n",
    "    animation_cues: Dict[str, str]\n",
    "    coach_feedback: Optional[Dict[str, Any]] = None\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    try:\n",
    "        auth = riva.client.Auth(uri=RIVA_SERVER)\n",
    "        riva.client.ASRService(auth)\n",
    "        riva_ok = True\n",
    "    except Exception:\n",
    "        riva_ok = False\n",
    "\n",
    "    model_ok = tokenizer is not None and adapter_model is not None\n",
    "    status_text = \"healthy\" if model_ok else \"degraded\"\n",
    "    health_payload = {\n",
    "        \"status\": status_text,\n",
    "        \"models_loaded\": model_ok,\n",
    "        \"ready_for_traffic\": model_ok,\n",
    "        \"riva_connected\": riva_ok,\n",
    "        \"api_auth_enabled\": API_AUTH_ENABLED,\n",
    "        \"api_auth_configured\": bool(API_KEY),\n",
    "        \"api_contract_version\": API_CONTRACT_VERSION,\n",
    "        \"guardrails_loaded\": guardrails_engine is not None,\n",
    "        \"firebase_creds_configured\": bool(FIREBASE_CREDS),\n",
    "    }\n",
    "    http_status = status.HTTP_200_OK if model_ok else status.HTTP_503_SERVICE_UNAVAILABLE\n",
    "    return JSONResponse(status_code=http_status, content=health_payload)\n",
    "\n",
    "\n",
    "@app.post(\"/v1/chat\", response_model=ChatResponse)\n",
    "async def process_chat(request: ChatRequest, _api_key: str = Depends(require_api_key)):\n",
    "    try:\n",
    "        if adapter_model is None or tokenizer is None:\n",
    "            raise HTTPException(status_code=503, detail=\"Model adapters are not initialized\")\n",
    "\n",
    "        session_ref = db.collection(\"sessions\").document(request.session_id)\n",
    "        session_state = session_ref.get().to_dict() or {}\n",
    "\n",
    "        input_guard = await enforce_guardrails_input(request.user_message)\n",
    "        if not input_guard[\"allowed\"]:\n",
    "            return ChatResponse(\n",
    "                response_text=input_guard[\"text\"],\n",
    "                emotion=\"neutral\",\n",
    "                animation_cues={\"gesture\": \"idle\"},\n",
    "                coach_feedback={\"safe\": False, \"reason\": input_guard[\"reason\"]}\n",
    "            )\n",
    "\n",
    "        mode = session_state.get(\"mode\", \"caregiver\")\n",
    "        primary_adapter = select_adapter_for_mode(mode)\n",
    "\n",
    "        prompt = f\"[SESSION: {request.session_id}] User: {input_guard['text']}\\\\nAssistant:\"\n",
    "        model_inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        model_inputs = {k: v.to(adapter_model.device) for k, v in model_inputs.items()}\n",
    "\n",
    "        async with inference_lock:\n",
    "            adapter_model.set_adapter(primary_adapter)\n",
    "            output = await asyncio.to_thread(\n",
    "                generate_tokens_sync,\n",
    "                adapter_model,\n",
    "                **model_inputs,\n",
    "                max_new_tokens=180,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        response_text = decoded.split(\"Assistant:\")[-1].strip() or \"I’m here to help with HPV vaccine communication practice.\"\n",
    "\n",
    "        output_guard = await enforce_guardrails_output(response_text)\n",
    "        response_text = output_guard[\"text\"]\n",
    "\n",
    "        audio_url = None\n",
    "        try:\n",
    "            auth = riva.client.Auth(uri=RIVA_SERVER)\n",
    "            tts = riva.client.SpeechSynthesisService(auth)\n",
    "            tts_resp = tts.synthesize(response_text, voice_name=\"English-US.Female-1\")\n",
    "            audio_url = \"data:audio/wav;base64,\" + base64.b64encode(tts_resp.audio).decode(\"utf-8\")\n",
    "        except Exception as riva_error:\n",
    "            logger.warning(\"Riva TTS unavailable: %s\", sanitize_for_storage(str(riva_error)))\n",
    "\n",
    "        coach_feedback_text = \"\"\n",
    "        try:\n",
    "            feedback_prompt = f\"Provide concise coaching feedback for this response: {response_text}\"\n",
    "            feedback_inputs = tokenizer(feedback_prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            feedback_inputs = {k: v.to(adapter_model.device) for k, v in feedback_inputs.items()}\n",
    "            async with inference_lock:\n",
    "                adapter_model.set_adapter(\"coach\")\n",
    "                feedback_tokens = await asyncio.to_thread(\n",
    "                    generate_tokens_sync,\n",
    "                    adapter_model,\n",
    "                    **feedback_inputs,\n",
    "                    max_new_tokens=80,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                )\n",
    "            coach_feedback_text = tokenizer.decode(feedback_tokens[0], skip_special_tokens=True)\n",
    "        finally:\n",
    "            async with inference_lock:\n",
    "                adapter_model.set_adapter(primary_adapter)\n",
    "\n",
    "        sanitized_user_message = sanitize_for_storage(request.user_message)\n",
    "        sanitized_response_text = sanitize_for_storage(response_text)\n",
    "        session_state[\"last_user_message\"] = sanitized_user_message\n",
    "        session_state[\"last_response\"] = sanitized_response_text\n",
    "        session_state[\"mode\"] = mode\n",
    "        session_state[\"phi_redaction\"] = \"presidio\"\n",
    "        session_state[\"phi_redaction_applied\"] = True\n",
    "        session_ref.set(session_state, merge=True)\n",
    "\n",
    "        return ChatResponse(\n",
    "            response_text=response_text,\n",
    "            audio_url=audio_url,\n",
    "            emotion=\"supportive\",\n",
    "            animation_cues={\"gesture\": \"speaking\", \"intensity\": \"low\"},\n",
    "            coach_feedback={\"safe\": output_guard[\"allowed\"], \"reason\": output_guard[\"reason\"], \"summary\": coach_feedback_text[:500]},\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.exception(\"/v1/chat failed after sanitization path: %s\", sanitize_for_storage(str(e)))\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "''').strip()\n",
    "main_content = (\n",
    "    main_content\n",
    "    .replace(\"{MODEL_DIR}\", str(MODEL_DIR))\n",
    "    .replace(\"{PUBAPPS_ROOT}\", str(PUBAPPS_ROOT))\n",
    "    .replace(\"{PUBAPP_ALLOWED_ORIGINS}\", str(PUBAPP_ALLOWED_ORIGINS))\n",
    ")\n",
    "\n",
    "BACKEND_DIR.mkdir(parents=True, exist_ok=True)\n",
    "main_py.write_text(main_content)\n",
    "print(f\"Wrote {main_py}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff5d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 C4/C5/M9/H2/H3/H5/H10/H11/H12/H13/H14 Smoke Test — Adapter/Auth/Config + Redaction + Contract + CORS + Guardrails + Async Inference + Health Readiness + Error Sanitization + Schema Constraints\n",
    "\n",
    "backend_text = main_py.read_text()\n",
    "\n",
    "required_markers = [\n",
    "    \"adapter_name=\\\"caregiver\\\"\",\n",
    "    \"load_adapter(ADAPTER_PATHS[\\\"coach\\\"], adapter_name=\\\"coach\\\")\",\n",
    "    \"load_adapter(ADAPTER_PATHS[\\\"supervisor\\\"], adapter_name=\\\"supervisor\\\")\",\n",
    "    \"adapter_model.set_adapter(primary_adapter)\",\n",
    "    \"adapter_model.set_adapter(\\\"coach\\\")\",\n",
    "    \"def require_api_key(\",\n",
    "    \"Header(default=None, alias=\\\"X-API-Key\\\")\",\n",
    "    \"Depends(require_api_key)\",\n",
    "    \"SPARC_FIREBASE_CREDS\",\n",
    "    \"SPARC_MODEL_BASE_PATH\",\n",
    "    \"SPARC_RIVA_SERVER\",\n",
    "    \"os.path.isfile(FIREBASE_CREDS)\",\n",
    "    \"from presidio_analyzer import AnalyzerEngine\",\n",
    "    \"from presidio_anonymizer import AnonymizerEngine\",\n",
    "    \"def sanitize_for_storage(\",\n",
    "    \"sanitized_user_message = sanitize_for_storage(request.user_message)\",\n",
    "    \"sanitized_response_text = sanitize_for_storage(response_text)\",\n",
    "    \"session_state[\\\"phi_redaction_applied\\\"] = True\",\n",
    "    \"API_CONTRACT_VERSION = \\\"v1\\\"\",\n",
    "    \"session_id: str = Field(..., min_length=1, max_length=128, pattern=r\\\"^[a-zA-Z0-9_-]+$\\\")\",\n",
    "    \"user_message: str = Field(..., min_length=1, max_length=10000)\",\n",
    "    \"audio_data: Optional[str] = Field(default=None, max_length=2_000_000)\",\n",
    "    \"api_contract_version\\\": API_CONTRACT_VERSION\",\n",
    "    \"CORS_ALLOWED_ORIGINS = [\",\n",
    "    \"CORS_ALLOW_CREDENTIALS = os.getenv(\\\"SPARC_CORS_ALLOW_CREDENTIALS\\\", \\\"false\\\")\",\n",
    "    \"allow_origins=CORS_ALLOWED_ORIGINS\",\n",
    "    \"allow_credentials=CORS_ALLOW_CREDENTIALS\",\n",
    "    \"from nemoguardrails import LLMRails, RailsConfig\",\n",
    "    \"load_guardrails_runtime()\",\n",
    "    \"enforce_guardrails_input(request.user_message)\",\n",
    "    \"enforce_guardrails_output(response_text)\",\n",
    "    \"guardrails_loaded\\\": guardrails_engine is not None\",\n",
    "    \"import asyncio\",\n",
    "    \"inference_lock = asyncio.Lock()\",\n",
    "    \"def generate_tokens_sync(\",\n",
    "    \"await asyncio.to_thread(\",\n",
    "    \"from fastapi.responses import JSONResponse\",\n",
    "    \"model_ok = tokenizer is not None and adapter_model is not None\",\n",
    "    \"ready_for_traffic\\\": model_ok\",\n",
    "    \"status.HTTP_503_SERVICE_UNAVAILABLE\",\n",
    "    \"return JSONResponse(status_code=http_status, content=health_payload)\",\n",
    "    \"logger.exception(\\\"/v1/chat failed after sanitization path: %s\\\", sanitize_for_storage(str(e)))\",\n",
    "    \"raise HTTPException(status_code=500, detail=\\\"Internal server error\\\")\",\n",
    "]\n",
    "\n",
    "missing = [marker for marker in required_markers if marker not in backend_text]\n",
    "assert not missing, f\"Missing required markers: {missing}\"\n",
    "\n",
    "assert \"caregiver_model = PeftModel.from_pretrained(base_model\" not in backend_text, \"Legacy shared-object adapter pattern remains\"\n",
    "assert \"coach_model = PeftModel.from_pretrained(base_model\" not in backend_text, \"Legacy shared-object adapter pattern remains\"\n",
    "assert \"supervisor_model = PeftModel.from_pretrained(base_model\" not in backend_text, \"Legacy shared-object adapter pattern remains\"\n",
    "assert \"async def process_chat(request: ChatRequest):\" not in backend_text, \"Endpoint still lacks auth dependency\"\n",
    "assert \"session_state[\\\"last_user_message\\\"] = request.user_message\" not in backend_text, \"Raw user message still persisted to Firebase\"\n",
    "assert \"session_state[\\\"last_response\\\"] = response_text\" not in backend_text, \"Raw response still persisted to Firebase\"\n",
    "assert \"user_transcript\" not in backend_text, \"Legacy request field still present\"\n",
    "assert \"allow_origins=[\\\"*\\\"]\" not in backend_text, \"Wildcard CORS origins remain configured\"\n",
    "assert \"allow_credentials=True\" not in backend_text, \"Credentialed wildcard CORS remains configured\"\n",
    "assert \"blocked = [\\\"politics\\\", \\\"election\\\", \\\"gambling\\\", \\\"crypto\\\", \\\"finance advice\\\"]\" not in backend_text, \"Legacy keyword blocklist remains configured\"\n",
    "assert \"output = adapter_model.generate(\" not in backend_text, \"Primary generation still blocks event loop\"\n",
    "assert \"feedback_tokens = adapter_model.generate(\" not in backend_text, \"Coach generation still blocks event loop\"\n",
    "assert \"\\\"models_loaded\\\": True\" not in backend_text, \"Health still hard-codes models_loaded=True\"\n",
    "assert \"detail=str(e)\" not in backend_text, \"Raw exception details still leak to client\"\n",
    "\n",
    "print(\"✅ C4/C5/M9/H2/H3/H5/H10/H11/H12/H13/H14 validation passed: named adapters, auth guard, env config, Presidio redaction, unified v1 API contract, safe CORS policy, runtime Guardrails pipeline, non-blocking async inference path, readiness-aware health behavior, sanitized client error responses, and strict request schema constraints are configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c289c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 H11 Load Test Script — Health Responsiveness Under Chat Load\n",
    "load_test_py = BACKEND_DIR / \"h11_health_load_test.py\"\n",
    "load_test_content = textwrap.dedent(\"\"\"\n",
    "import os\n",
    "import time\n",
    "import statistics\n",
    "import concurrent.futures\n",
    "\n",
    "import requests\n",
    "\n",
    "BASE_URL = os.getenv(\"SPARC_BASE_URL\", \"http://localhost:8000\")\n",
    "API_KEY = os.getenv(\"SPARC_API_KEY\", \"\")\n",
    "HEADERS = {\"X-API-Key\": API_KEY} if API_KEY else {}\n",
    "CHAT_PAYLOAD = {\n",
    "    \"session_id\": \"h11-load\",\n",
    "    \"user_message\": \"Help me discuss HPV vaccines with a hesitant caregiver.\"\n",
    "}\n",
    "\n",
    "\n",
    "def post_chat() -> int:\n",
    "    response = requests.post(f\"{BASE_URL}/v1/chat\", json=CHAT_PAYLOAD, headers=HEADERS, timeout=120)\n",
    "    return response.status_code\n",
    "\n",
    "\n",
    "def ping_health() -> float:\n",
    "    start = time.perf_counter()\n",
    "    response = requests.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    response.raise_for_status()\n",
    "    return (time.perf_counter() - start) * 1000\n",
    "\n",
    "\n",
    "health_latencies = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=12) as pool:\n",
    "    chat_futures = [pool.submit(post_chat) for _ in range(30)]\n",
    "    for _ in range(60):\n",
    "        health_latencies.append(ping_health())\n",
    "        time.sleep(0.2)\n",
    "    chat_statuses = [f.result() for f in chat_futures]\n",
    "\n",
    "health_p95 = statistics.quantiles(health_latencies, n=20)[18] if len(health_latencies) >= 20 else max(health_latencies)\n",
    "health_success_ratio = sum(1 for latency in health_latencies if latency < 1500) / len(health_latencies)\n",
    "\n",
    "assert all(code in (200, 401, 422) for code in chat_statuses), f\"Unexpected chat status codes: {sorted(set(chat_statuses))}\"\n",
    "assert health_success_ratio >= 0.99, f\"Health responsiveness dropped below target: {health_success_ratio:.3f}\"\n",
    "assert health_p95 < 1500, f\"Health p95 latency too high under chat load: {health_p95:.1f}ms\"\n",
    "\n",
    "print(f\"✅ H11 load test passed: /health p95={health_p95:.1f}ms, success_ratio={health_success_ratio:.3f}\")\n",
    "\"\"\").strip()\n",
    "load_test_py.write_text(load_test_content)\n",
    "print(f\"Wrote {load_test_py}\")\n",
    "print(\"Run with: python h11_health_load_test.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8de462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Create systemd user service for FastAPI\n",
    "systemd_dir = Path.home() / '.config/systemd/user'\n",
    "systemd_dir.mkdir(parents=True, exist_ok=True)\n",
    "service_file = systemd_dir / 'sparc-backend.service'\n",
    "service_content = textwrap.dedent(f\"\"\"\n",
    "[Unit]\n",
    "Description=SPARC-P FastAPI Backend\n",
    "After=network.target riva-server.service\n",
    "Requires=riva-server.service\n",
    "\n",
    "[Service]\n",
    "Type=simple\n",
    "Environment=PATH={CONDA_ENV}/bin:/usr/bin\n",
    "Environment=PYTHONUNBUFFERED=1\n",
    "WorkingDirectory={BACKEND_DIR}\n",
    "ExecStart={CONDA_ENV}/bin/uvicorn main:app --host 0.0.0.0 --port 8000 --workers {UVICORN_WORKERS}\n",
    "Restart=always\n",
    "RestartSec=10\n",
    "\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "\"\"\").strip()\n",
    "service_file.write_text(service_content)\n",
    "print(f\"Wrote {service_file}\")\n",
    "print(service_content)\n",
    "print(f\"Configured uvicorn workers: {UVICORN_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7878243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Enable backend service\n",
    "run(\"systemctl --user daemon-reload\")\n",
    "run(\"systemctl --user enable --now sparc-backend\")\n",
    "run(\"systemctl --user status sparc-backend --no-pager\", check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53760c43",
   "metadata": {},
   "source": [
    "## 7. Validation Checks\n",
    "Set `EXECUTE = True` before running these checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89137975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Health and service checks\n",
    "run(\"curl -s http://localhost:8000/health\", check=False)\n",
    "run(\"journalctl --user -u riva-server -n 50 --no-pager\", check=False)\n",
    "run(\"journalctl --user -u sparc-backend -n 50 --no-pager\", check=False)\n",
    "run(f\"ls -lh {MODEL_DIR}\", check=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
