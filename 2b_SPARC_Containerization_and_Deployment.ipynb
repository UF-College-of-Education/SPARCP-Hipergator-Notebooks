{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c351f3",
   "metadata": {},
   "source": [
    "# SPARC Containerization and Deployment\n",
    "\n",
    "## 1.0 Introduction\n",
    "This notebook covers packaging SPARC services as portable containers and preparing both local Podman validation and HiPerGator production deployment artifacts.\n",
    "\n",
    "### 1.1 Objectives\n",
    "1. **Containerize**: Build images for MAS backend, Unity Linux server runtime, and Signaling Server.\n",
    "2. **Orchestrate**: Validate local Podman pod networking for server-side rendering (Pixel Streaming).\n",
    "3. **Deploy**: Generate production SLURM artifacts for HiPerGator-compatible backend workflows.\n",
    "\n",
    "### 1.2 Introduction Diagram\n",
    "![Introduction](./images/notebook_2_-_section_1.png)\n",
    "\n",
    "Introduction: This section defines the containerization objectives for the SPARC stack. We now support server-side rendering for thin clients by introducing a Unity Linux server runtime and a signaling container in addition to the backend services.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.0 Containerization (Docker/Podman -> Apptainer)\n",
    "We develop with Docker/Podman and deploy with Apptainer on HPC when needed.\n",
    "\n",
    "### 2.0 Container Build Strategy Diagram\n",
    "![Container Build Strategy](./images/notebook_2_-_section_2.png)\n",
    "\n",
    "Container Build Strategy: The flow uses secure, minimal runtime images. Build steps compile dependencies in dedicated stages, then copy only required artifacts into runtime images.\n",
    "\n",
    "### 2.1 Dockerfile Definition\n",
    "\n",
    "This section provides image definitions for three build targets:\n",
    "1. **MAS Backend** (`Dockerfile.mas`)\n",
    "2. **Unity Linux Server Build** (`Dockerfile.unity-server`)\n",
    "3. **WebRTC Signaling Server** (`Dockerfile.signaling`)\n",
    "\n",
    "Canonical build-context artifacts used by these Dockerfiles:\n",
    "- `requirements.txt` (MAS Python dependencies)\n",
    "- `artifacts/unity/LinuxServer/` (Unity Linux server build output)\n",
    "- `artifacts/signaling/` (Render Streaming signaling server source)\n",
    "\n",
    "### 2.2 Dockerfile for Multi-Agent System (MAS)\n",
    "\n",
    "**Note on Conda vs Containers**: On HiPerGator and PubApps you can deploy with conda environments, but containers are useful for portability and repeatability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ea902",
   "metadata": {},
   "source": [
    "Two files are produced — `requirements.txt` (the Python dependency list for the MAS backend) and `Dockerfile.mas` (the container recipe) — and a validation check confirms that the Unity and signaling build artifacts are present before continuing.\n",
    "\n",
    "- `create_requirements_file()` writes all required Python libraries (AI models, speech, PII redaction, vector search, etc.) to a plain text file that Docker will use during the image build.\n",
    "- `create_mas_dockerfile()` writes a two-stage Dockerfile: the \"builder\" stage installs all heavy dependencies; the \"runtime\" stage copies only the final packages into a small, clean image that deploys faster and has a smaller attack surface.\n",
    "- `validate_container_artifacts()` checks that the `artifacts/unity/LinuxServer/` and `artifacts/signaling/` directories exist before proceeding. If either is missing, it raises an error so you know exactly what's needed before attempting a build.\n",
    "- The `Dockerfile.mas` exposes port `8000` and launches the backend with `uvicorn`, the high-performance async web server used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13d545",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 2.2 Dockerfile for Multi-Agent System (MAS)\n",
    "from pathlib import Path\n",
    "\n",
    "UNITY_BUILD_DIR = Path(\"artifacts/unity/LinuxServer\")\n",
    "SIGNALING_DIR = Path(\"artifacts/signaling\")\n",
    "\n",
    "def create_requirements_file():\n",
    "    requirements = \"\"\"\n",
    "fastapi\n",
    "uvicorn[standard]\n",
    "pydantic>=2.5.0\n",
    "numpy>=1.24.0\n",
    "aiofiles\n",
    "websockets\n",
    "python-multipart\n",
    "transformers>=4.36.0\n",
    "accelerate>=0.25.0\n",
    "tokenizers>=0.15.0\n",
    "bitsandbytes>=0.41.0\n",
    "peft>=0.7.0\n",
    "langchain>=0.1.0\n",
    "langchain-community>=0.0.13\n",
    "langchain-openai>=0.0.5\n",
    "langchain-chroma>=0.1.0\n",
    "langgraph>=0.0.26\n",
    "nvidia-riva-client>=2.14.0\n",
    "nemoguardrails>=0.5.0\n",
    "chromadb>=0.4.22\n",
    "presidio-analyzer>=2.2.33\n",
    "presidio-anonymizer>=2.2.33\n",
    "firebase-admin>=6.2.0\n",
    "python-jose[cryptography]\n",
    "python-dotenv\n",
    "grpcio\n",
    "grpcio-tools\n",
    "\"\"\".strip()\n",
    "    Path(\"requirements.txt\").write_text(requirements + \"\\n\", encoding=\"utf-8\")\n",
    "    print(\"Created requirements.txt\")\n",
    "\n",
    "def validate_container_artifacts():\n",
    "    missing = []\n",
    "    if not Path(\"requirements.txt\").exists():\n",
    "        missing.append(\"requirements.txt\")\n",
    "    if not UNITY_BUILD_DIR.exists():\n",
    "        missing.append(str(UNITY_BUILD_DIR))\n",
    "    if not SIGNALING_DIR.exists():\n",
    "        missing.append(str(SIGNALING_DIR))\n",
    "    if missing:\n",
    "        raise FileNotFoundError(f\"Missing build artifacts: {missing}\")\n",
    "\n",
    "def create_mas_dockerfile():\n",
    "    dockerfile_content = \"\"\"\n",
    "# --- Build Stage ---\n",
    "FROM python:3.11-slim as builder\n",
    "WORKDIR /app\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY requirements.txt ./\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY . .\n",
    "\n",
    "# --- Runtime Stage ---\n",
    "FROM python:3.11-slim\n",
    "WORKDIR /app\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    curl \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages\n",
    "COPY --from=builder /app /app\n",
    "\n",
    "EXPOSE 8000\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "    \"\"\"\n",
    "    with open(\"Dockerfile.mas\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(dockerfile_content.strip())\n",
    "    print(\"Created Dockerfile.mas\")\n",
    "\n",
    "create_requirements_file()\n",
    "create_mas_dockerfile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c393f",
   "metadata": {},
   "source": [
    "### 2.3 Dockerfile for Unity Linux Server (Render Streaming)\n",
    "\n",
    "This image packages a Unity Linux player build for server-side rendering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db140af1",
   "metadata": {},
   "source": [
    "`Dockerfile.unity-server` is a container recipe that packages the compiled Unity Linux Server application so it can run headlessly on a GPU-equipped server and stream its rendered output over WebRTC to users’ browsers.\n",
    "\n",
    "Key choices in this Dockerfile:\n",
    "- **Base image `nvidia/opengl:1.2-glvnd-runtime-ubuntu20.04`:** Unity's Linux player needs OpenGL libraries to render graphics, and it needs access to the NVIDIA GPU. This base image provides both — without it, Unity would crash immediately on a headless server.\n",
    "- **System dependencies:** Libraries like `libasound2` (audio), `libglu1-mesa` (3D graphics utilities), and `libxi6` (input) are required by the Unity runtime even in server mode.\n",
    "- **`COPY artifacts/unity/LinuxServer/ /app/`:** Copies your compiled Unity build (produced by Unity Editor → Build Settings → Linux → Server Build) into the image. You must place these files in `artifacts/unity/LinuxServer/` before building.\n",
    "- **`-batchmode -force-vulkan`:** Runs Unity without a display window (headless) and forces the Vulkan graphics API, which is required for GPU-accelerated server-side rendering on Linux.\n",
    "- Port `8080` is where Unity's Render Streaming plugin will serve WebRTC signaling traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42303c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 2.3 Dockerfile for Unity Linux Server Build\n",
    "def create_unity_server_dockerfile():\n",
    "    dockerfile_content = \"\"\"\n",
    "FROM nvidia/opengl:1.2-glvnd-runtime-ubuntu20.04\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\\\n",
    "    libnss3 \\\\\n",
    "    libxss1 \\\\\n",
    "    libasound2 \\\\\n",
    "    libglu1-mesa \\\\\n",
    "    libxi6 \\\\\n",
    "    ca-certificates \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy Linux server build output from canonical artifact directory\n",
    "COPY artifacts/unity/LinuxServer/ /app/\n",
    "\n",
    "RUN chmod +x /app/SPARC-P.x86_64\n",
    "\n",
    "# Typical Render Streaming launch flags.\n",
    "# Use -batchmode/-nographics for headless workflows, or -force-vulkan when package requires GPU rendering context.\n",
    "CMD [\"/app/SPARC-P.x86_64\", \"-logFile\", \"/dev/stdout\", \"-batchmode\", \"-force-vulkan\"]\n",
    "    \"\"\"\n",
    "    with open(\"Dockerfile.unity-server\", \"w\") as f:\n",
    "        f.write(dockerfile_content.strip())\n",
    "    print(\"Created Dockerfile.unity-server\")\n",
    "\n",
    "create_unity_server_dockerfile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b7a71",
   "metadata": {},
   "source": [
    "### 2.4 Dockerfile for WebRTC Signaling Server (Node.js)\n",
    "\n",
    "Use the signaling server from the Unity Render Streaming package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd44ad",
   "metadata": {},
   "source": [
    "`Dockerfile.signaling` is a container recipe for a lightweight Node.js server that acts as the matchmaker between users’ browsers and the Unity GPU renderer, enabling the WebRTC peer connection that carries the live video stream.\n",
    "\n",
    "How it works:\n",
    "- **Base image `node:20-alpine`:** Alpine Linux is a minimal OS (~5 MB), keeping this container very small since a signaling server doesn't need a GPU or heavy libraries — just a Node.js runtime.\n",
    "- **`COPY artifacts/signaling/ /app/`:** Copies the signaling server source files (typically from the Unity Render Streaming package — `package.json`, `server.js`, etc.) into the container. Place these in `artifacts/signaling/` before building.\n",
    "- **`npm ci --omit=dev`:** Installs only production dependencies (no test tools or dev utilities), making the image as small as possible.\n",
    "- **Ports 8080 and 8888:** Port 8080 is the main HTTP/WebSocket endpoint where browsers connect to negotiate WebRTC. Port 8888 is an optional metrics or admin endpoint provided by some versions of the Unity signaling package.\n",
    "- After writing the Dockerfile, `validate_container_artifacts()` runs immediately to confirm all three required artifact directories (`requirements.txt`, `artifacts/unity/LinuxServer/`, `artifacts/signaling/`) exist — halting with a clear error if anything is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860a6db",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 2.4 Dockerfile for Signaling Server\n",
    "def create_signaling_dockerfile():\n",
    "    dockerfile_content = \"\"\"\n",
    "FROM node:20-alpine\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy signaling source from canonical artifact directory\n",
    "COPY artifacts/signaling/ /app/\n",
    "\n",
    "RUN npm ci --omit=dev\n",
    "\n",
    "EXPOSE 8080 8888\n",
    "\n",
    "# 8080: HTTP/WebSocket signaling endpoint\n",
    "# 8888: Optional metrics/admin endpoint if enabled by your signaling package\n",
    "CMD [\"node\", \"server.js\", \"--httpPort\", \"8080\"]\n",
    "    \"\"\"\n",
    "    with open(\"Dockerfile.signaling\", \"w\") as f:\n",
    "        f.write(dockerfile_content.strip())\n",
    "    print(\"Created Dockerfile.signaling\")\n",
    "\n",
    "create_signaling_dockerfile()\n",
    "validate_container_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc77275",
   "metadata": {},
   "source": [
    "### 2.5 Build Commands (Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43db183",
   "metadata": {},
   "source": [
    "Three `podman build` commands build the three container images from the Dockerfiles created above. Run these in your terminal (not as Python — they are shell commands) after the Dockerfiles and artifacts are in place.\n",
    "\n",
    "- `podman build -f Dockerfile.mas` → builds the AI backend image (Python 3.11, all ML libraries)\n",
    "- `podman build -f Dockerfile.unity-server` → builds the Unity renderer image (Ubuntu + NVIDIA OpenGL)  \n",
    "- `podman build -f Dockerfile.signaling` → builds the signaling server image (Node.js Alpine)\n",
    "\n",
    "Each image is tagged with the `sparc/` namespace and `:latest` so they can be referenced by the Podman pod commands in the next sections. Building all three can take **5–20 minutes** depending on your internet connection and CPU speed, as the ML libraries alone are several gigabytes.\n",
    "\n",
    "> **Prerequisite:** Run these only after the `artifacts/unity/LinuxServer/` and `artifacts/signaling/` directories have been populated (see the staging step below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539622a4",
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "podman build -f Dockerfile.mas -t sparc/mas-server:latest .\n",
    "podman build -f Dockerfile.unity-server -t sparc/unity-server:latest .\n",
    "podman build -f Dockerfile.signaling -t sparc/signaling-server:latest ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb5b7d",
   "metadata": {},
   "source": [
    "Artifact staging reference (run before `podman build`):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999adff8",
   "metadata": {},
   "source": [
    "Two empty directories — `artifacts/unity/LinuxServer/` and `artifacts/signaling/` — are created here for the Dockerfiles to reference when building images. The `mkdir -p` flag means \"make the full path and don't fail if it already exists.\"\n",
    "\n",
    "After this step, you need to **manually copy files into these directories** before running `podman build`:\n",
    "- Copy your **Unity Linux Server build output** (the `SPARC-P.x86_64` binary and its accompanying `_Data/` directory) into `artifacts/unity/LinuxServer/`\n",
    "- Copy the **signaling server package files** (typically `package.json`, `package-lock.json`, and `server.js` from the Unity Render Streaming npm package) into `artifacts/signaling/`\n",
    "\n",
    "The `validate_container_artifacts()` function (called at the end of the signaling Dockerfile cell) will check these directories exist before allowing the build to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f5e27",
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "mkdir -p artifacts/unity/LinuxServer artifacts/signaling\n",
    "# Place Unity Linux server build output in artifacts/unity/LinuxServer/\n",
    "# Place signaling package files (e.g., package.json, server.js) in artifacts/signaling/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2509875",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.0 Local Development with Podman\n",
    "Podman pods allow local validation of the same localhost service mesh used in deployment.\n",
    "\n",
    "### 3.0 Local Development Pod Diagram\n",
    "![Local Development Pod](./images/notebook_2_-_section_3.png)\n",
    "\n",
    "Local Development Pod (Podman): The local pod now includes MAS backend, Riva, Unity Linux server renderer, and signaling server. The browser receives a live video stream over WebRTC rather than loading a local Unity WebGL build. Audio2Face is removed from this architecture.\n",
    "\n",
    "### 3.1 Podman Local Workflow\n",
    "\n",
    "For local development, run all core services in one pod so they share localhost routing:\n",
    "- Unity renderer -> localhost services (Riva + backend APIs)\n",
    "- Signaling server -> browser negotiation path\n",
    "- Browser -> receives WebRTC stream from Unity runtime\n",
    "\n",
    "### 3.2 Podman Workflow (Reference Commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a0299",
   "metadata": {},
   "source": [
    "The complete set of Podman commands needed to launch all five SPARC-P components simultaneously is printed below, sharing a common network so they can communicate. Nothing is executed automatically — copy and paste these into your terminal.\n",
    "\n",
    "What gets started and why:\n",
    "1. **`podman pod create`** — creates a virtual network namespace called `sparc-avatar` where everything shares `localhost`. The port mappings expose the API (`8000`), signaling (`8080`), STUN/TURN (`3478` UDP), and a block of UDP ports (`49152–49200`) that WebRTC uses for actual video streaming.\n",
    "2. **MAS server** — the AI orchestration backend (all three SPARC-P agents + FastAPI endpoints).\n",
    "3. **Riva server** — NVIDIA's speech AI service that converts spoken audio to text (ASR) and text back to speech (TTS). The `--device nvidia.com/gpu=all` flag passes the local GPU through to the container.\n",
    "4. **Unity server** — the 3D avatar renderer. It receives a `SIGNALING_URL` environment variable pointing to the signaling server so it knows where to connect for WebRTC negotiation. Also needs GPU access.\n",
    "5. **Signaling server** — the WebRTC rendezvous point. `PUBLIC_HOST=localhost` tells it to advertise `localhost` as the ICE candidate address (appropriate for local testing; change to your public hostname for remote access).\n",
    "\n",
    "> **Expected result:** After all containers start, open your browser to `http://localhost:8080` to view the live-rendered digital human avatar stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e951f7c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 3.2 Podman Workflow (Reference Commands)\n",
    "podman_commands = \"\"\"\n",
    "# 1. Create Pod with signaling + API ports and WebRTC UDP range\n",
    "podman pod create --name sparc-avatar \\\\\n",
    "  -p 8000:8000 \\\\\n",
    "  -p 8080:8080 \\\\\n",
    "  -p 3478:3478/udp \\\\\n",
    "  -p 49152-49200:49152-49200/udp\n",
    "\n",
    "# 2. Run MAS Server\n",
    "podman run -d --pod sparc-avatar --name mas-server sparc/mas-server:latest\n",
    "\n",
    "# 3. Run Riva Server\n",
    "podman run -d --pod sparc-avatar --name riva-server \\\\\n",
    "  --device nvidia.com/gpu=all \\\\\n",
    "  nvcr.io/nvidia/riva/riva-speech:2.16.0-server\n",
    "\n",
    "# 4. Run Unity Linux Render Streaming Server\n",
    "podman run -d --pod sparc-avatar --name unity-server \\\\\n",
    "  --device nvidia.com/gpu=all \\\\\n",
    "  -e SIGNALING_URL=ws://localhost:8080 \\\\\n",
    "  sparc/unity-server:latest\n",
    "\n",
    "# 5. Run Signaling Server\n",
    "podman run -d --pod sparc-avatar --name signaling-server \\\\\n",
    "  -e PUBLIC_HOST=localhost \\\\\n",
    "  sparc/signaling-server:latest\n",
    "\"\"\"\n",
    "print(podman_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6918ed47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.0 Production Deployment on HiPerGator\n",
    "Deploy persistent backend services using SLURM and Apptainer where required.\n",
    "\n",
    "### 4.0 Production Deployment Diagram\n",
    "![Production Deployment](./images/notebook_2_-_section_4.png)\n",
    "\n",
    "Production Deployment (SLURM): This diagram represents the HiPerGator scheduling path for backend services. PubApps runtime orchestration is handled by Podman + systemd user services.\n",
    "\n",
    "### 4.1 Building SIF Images\n",
    "\n",
    "HiPerGator uses Apptainer, which requires Singularity Image Format (`.sif`) files.\n",
    "\n",
    "### 4.2 Build SIF Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b86cf",
   "metadata": {},
   "source": [
    "A documentation placeholder prints a reminder that `.sif` (Singularity Image Format) files must be built before deploying to HiPerGator. The actual build commands are commented out because they need to run in a HiPerGator login node terminal with the Apptainer module loaded.\n",
    "\n",
    "Why this step is necessary: HiPerGator's compute nodes cannot use Docker or Podman directly. They require Apptainer (formerly Singularity), which uses self-contained `.sif` image files. You build these once from your Docker images on a login node and store them in `/blue/` — then any compute node can run them without an internet connection.\n",
    "\n",
    "> **To actually build the images:** SSH into a HiPerGator login node, run `module load apptainer`, uncomment the three `apptainer build` lines, and execute them. Each build can take 10–30 minutes depending on image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4aace",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 4.2 Build SIF Images\n",
    "# !module load apptainer\n",
    "# !apptainer build mas_server.sif docker-daemon://sparc/mas-server:latest\n",
    "print(\"Build SIF images from Docker sources before production deployment on HPG.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee8d4b",
   "metadata": {},
   "source": [
    "### 4.3 Production Service Launch\n",
    "\n",
    "This function generates a baseline `sparc_production.slurm` script for backend execution on HiPerGator.\n",
    "\n",
    "### 4.4 Production SLURM Script Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c060461",
   "metadata": {},
   "source": [
    "`sparc_production.slurm` is the SLURM batch script for the Pixel Streaming variant of SPARC-P. This is the most resource-intensive configuration because it must simultaneously run the AI backend, Riva speech AI, and the Unity GPU renderer on a single node.\n",
    "\n",
    "What makes this different from the standard deployment SLURM script:\n",
    "- **4 GPUs, 32 CPU cores, 256 GB RAM** — Unity's server-side rendering requires substantial GPU memory alongside the language models. This resource request reflects the combined needs of all services.\n",
    "- **MAS server only** — this script launches just the `mas_server.sif` container with `uvicorn`. The Riva and Unity servers are typically managed separately via Podman Quadlet units (see Notebooks 4 and 4b), as SLURM job termination would kill all co-located services.\n",
    "- **14-day time limit** — extended from the 7-day standard to reduce how often the job needs to be resubmitted.\n",
    "- **Background launch + `wait`** — starts the MAS server asynchronously with `&` and `wait` keeps the SLURM job alive until the process exits or the time limit is reached.\n",
    "\n",
    "> **After running:** Transfer `sparc_production.slurm` to HiPerGator and deploy with `sbatch sparc_production.slurm`. Monitor with `squeue -u $USER`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499dbb8",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 4.4 Production SLURM Script Generator\n",
    "def generate_production_script():\n",
    "    script_content = \"\"\"\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=sparc-production-service\n",
    "#SBATCH --partition=hpg-ai\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --gpus-per-task=4\n",
    "#SBATCH --cpus-per-task=32\n",
    "#SBATCH --mem=256gb\n",
    "#SBATCH --time=14-00:00:00\n",
    "#SBATCH --output=sparc_service_%j.log\n",
    "\n",
    "module purge\n",
    "module load apptainer\n",
    "\n",
    "MAS_SIF=\"/blue/jasondeanarnold/SPARCP/containers/mas_server.sif\"\n",
    "\n",
    "echo \"Starting MAS...\"\n",
    "apptainer exec --nv ${MAS_SIF} uvicorn main:app --host 0.0.0.0 --port 8000 &\n",
    "\n",
    "wait\n",
    "    \"\"\"\n",
    "    with open(\"sparc_production.slurm\", \"w\") as f:\n",
    "        f.write(script_content.strip())\n",
    "    print(\"Generated sparc_production.slurm\")\n",
    "\n",
    "generate_production_script()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86d2604",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook now covers both classic backend containerization and the new server-side rendering container artifacts:\n",
    "\n",
    "1. **MAS Backend Container** for API/model orchestration.\n",
    "2. **Unity Linux Server Container** for GPU-rendered server-side scene streaming.\n",
    "3. **Node Signaling Container** for WebRTC negotiation.\n",
    "4. **Podman Pod Topology** for localhost service routing and browser video streaming.\n",
    "5. **HiPerGator Production Script** for backend workflows where Apptainer + SLURM are required.\n",
    "\n",
    "The architecture removes Audio2Face from the deployment path and aligns local container artifacts with the PubApps Pixel Streaming runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
