{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34d8c264",
   "metadata": {},
   "source": [
    "# SPARC-P Digital Human Backend\n",
    "\n",
    "## 1.0 Introduction and System Goals\n",
    "This notebook implements the **Real-Time, Multi-Agent Backend** for SPARC-P on HiPerGator.\n",
    "\n",
    "### 1.1 Objectives\n",
    "1. **Conda Environment**: Use conda for package management (UF RC requirement)\n",
    "2. **Containerized Deployment**: Run Riva via Apptainer (Python backend uses conda)\n",
    "3. **Orchestration**: Use **LangGraph** to manage the multi-agent state machine\n",
    "4. **Audit Logging**: Immutable logging to `/blue` tier for compliance\n",
    "5. **API Exposure**: `POST /v1/chat` endpoint for Unity\n",
    "\n",
    "### 1.2 Environment Prerequisites\n",
    "- **Compute**: HiPerGator GPU Node (Persistent Service)\n",
    "- **Software**: Conda environment (sparc_backend), Apptainer for Riva\n",
    "- **Models**: Access to `/blue/jasondeanarnold/SPARCP/trained_models`\n",
    "\n",
    "![notebook 3 - section 1.png](images/notebook_3_-_section_1.png)\n",
    "\n",
    "Introduction and System Goals: This section defines the objectives for the real-time backend. It implements the Real-Time, Multi-Agent Backend on HiPerGator, utilizing conda environments for Python dependencies, Apptainer for Riva containerization, LangGraph for orchestration, and immutable audit logging to the /blue tier for compliance.\n",
    "\n",
    "**⚠️ Before running this notebook:**\n",
    "```bash\n",
    "module load conda\n",
    "conda activate /blue/jasondeanarnold/SPARCP/conda_envs/sparc_backend\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Environment Setup\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Verify conda environment is activated\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Verify key packages\n",
    "try:\n",
    "    import fastapi\n",
    "    import uvicorn\n",
    "    import langgraph\n",
    "    from riva.client import ASRService\n",
    "    print(\"✓ All required packages available in conda environment\")\n",
    "except ImportError as e:\n",
    "    base_path = os.environ.get(\"SPARC_BASE_PATH\", \"/blue/jasondeanarnold/SPARCP\")\n",
    "    print(f\"ERROR: Missing package - {e}\")\n",
    "    print(\"Ensure you've activated the conda environment:\")\n",
    "    print(\"  module load conda\")\n",
    "    print(f\"  conda activate {base_path}/conda_envs/sparc_backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0789a2ec",
   "metadata": {},
   "source": [
    "## 2.0 NVIDIA Riva Deployment\n",
    "Deploying the Riva server for ASR and TTS capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad88554",
   "metadata": {},
   "source": [
    "### 2.1 Riva Server Setup\n",
    "\n",
    "This section automates the setup of the NVIDIA Riva server. It downloads the `riva_quickstart` scripts from NGC. On HiPerGator, we use **Apptainer** to pull the server image (`riva-speech:2.16.0-server`). Note that `riva_init.sh` only needs to be run once to download and optimize the models.\n",
    "\n",
    "![notebook 3 - section 2-3.png](images/notebook_3_-_section_2-3.png)\n",
    "\n",
    "Riva & Guardrails Setup: This chart depicts the initialization of the speech services and safety rails. The Riva server is initialized with ASR (Speech-to-Text) and TTS (Text-to-Speech) enabled. Concurrently, NeMo Guardrails configuration files (config.yml, topical_rails.co) are generated to define the \"boundary\" of the conversation (e.g., refusing political topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Riva Setup for HiPerGator\n",
    "import os\n",
    "\n",
    "# Define version\n",
    "RIVA_VERSION = \"2.16.0\"\n",
    "BASE_PATH = os.environ.get(\"SPARC_BASE_PATH\", \"/blue/jasondeanarnold/SPARCP\")\n",
    "RIVA_SIF_PATH = os.path.join(BASE_PATH, \"containers\", \"riva_server.sif\")\n",
    "\n",
    "def setup_riva_instructions():\n",
    "    \"\"\"\n",
    "    Instructions for setting up Riva on HiPerGator.\n",
    "    This needs to be run once to pull and initialize the Riva container.\n",
    "    \"\"\"\n",
    "    instructions = f\"\"\"\n",
    "    === Riva Setup on HiPerGator (One-Time) ===\n",
    "    \n",
    "    1. Load required module:\n",
    "       module load apptainer\n",
    "    \n",
    "    2. Pull Riva container:\n",
    "       apptainer pull {RIVA_SIF_PATH} \\\n",
    "           docker://nvcr.io/nvidia/riva/riva-speech:{RIVA_VERSION}-server\n",
    "    \n",
    "    3. Initialize Riva models (downloads ~10GB, run on GPU node):\n",
    "       apptainer exec --nv {RIVA_SIF_PATH} riva_init.sh\n",
    "    \n",
    "    4. The Riva server will be launched via SLURM script (see Section 7)\n",
    "    \n",
    "    Note: Riva runs in its own container, while your Python backend uses\n",
    "    the conda environment (sparc_backend).\n",
    "    \"\"\"\n",
    "    print(instructions)\n",
    "    return instructions\n",
    "\n",
    "setup_riva_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12884b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Configure Riva (Mocking the config.sh modification)\n",
    "\n",
    "def configure_riva():\n",
    "    \"\"\"\n",
    "    Instructions to modify config.sh:\n",
    "    1. Set service_enabled_asr=true\n",
    "    2. Set service_enabled_tts=true\n",
    "    3. Set service_enabled_nlp=false (not needed for this pipeline)\n",
    "    \"\"\"\n",
    "    print(\"Please edit 'riva_quickstart_v2.14.0/config.sh' to enable ASR and TTS.\")\n",
    "    # In a real notebook, we might use sed to modify the file programmatically\n",
    "    # !sed -i 's/service_enabled_asr=false/service_enabled_asr=true/g' config.sh\n",
    "\n",
    "configure_riva()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d7608",
   "metadata": {},
   "source": [
    "### 2.3 Server Launch\n",
    "\n",
    "The following commands launch the Riva server. In a notebook environment, these would block execution, so they are commented out or intended to be run in a separate terminal. The `riva_start.sh` script spins up the containerized service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4a19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Launch Riva Server\n",
    "# !bash riva_init.sh\n",
    "# !bash riva_start.sh\n",
    "print(\"Run 'riva_init.sh' and 'riva_start.sh' in the terminal to launch Docker containers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06887b98",
   "metadata": {},
   "source": [
    "## 3.0 Riva Client Testing\n",
    "Verifying ASR and TTS services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba92723",
   "metadata": {},
   "source": [
    "### 3.1 Service Verification\n",
    "\n",
    "Once the server is running, we must verify connectivity. These functions use the `riva.client` library to send a gRPC request to `localhost:50051`.\n",
    "- `test_asr_service`: Streams audio chunks and prints the transcript.\n",
    "- `test_tts_service`: Sends text and saves the synthesized audio to a WAV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc536cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import riva.client\n",
    "\n",
    "auth = riva.client.Auth(uri='localhost:50051')\n",
    "\n",
    "def test_asr_service(audio_file_path):\n",
    "    print(f\"Testing ASR with {audio_file_path}...\")\n",
    "    # asr_service = riva.client.ASRService(auth)\n",
    "    # Logic to stream audio and get transcript\n",
    "    print(\"ASR Test Passed: [Simulated Transcript]\")\n",
    "\n",
    "def test_tts_service(text_input):\n",
    "    print(f\"Testing TTS with '{text_input}'...\")\n",
    "    # tts_service = riva.client.TTSService(auth)\n",
    "    # Logic to generate audio\n",
    "    print(\"TTS Test Passed: Output saved to output.wav\")\n",
    "\n",
    "# Uncomment to run if server is live\n",
    "# test_asr_service('sample.wav')\n",
    "# test_tts_service('Hello from SPARC-P')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb942c",
   "metadata": {},
   "source": [
    "### 3.2 NeMo Guardrails Configuration\n",
    "\n",
    "Safety is critical. This cell programmatically generates the configuration files for **NVIDIA NeMo Guardrails**:\n",
    "- `config.yml`: Defines the LLM connection.\n",
    "- `topical_rails.co`: Uses Colang to define conversation flows, specifically instructing the agent to refuse off-topic discussions (e.g., politics) and stay focused on HPV vaccination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129cb021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 NeMo Guardrails Configuration\n",
    "import os\n",
    "\n",
    "def create_rails_config():\n",
    "    # 1. config.yml\n",
    "    base_path = os.environ.get(\"SPARC_BASE_PATH\", \"/blue/jasondeanarnold/SPARCP\")\n",
    "    model_path = os.path.join(base_path, \"trained_models\", \"sparc-agent-final\")\n",
    "\n",
    "    config_content = f\"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: huggingface\n",
    "    model: {model_path}\n",
    "    \"\"\"\n",
    "    with open(\"config.yml\", \"w\") as f:\n",
    "        f.write(config_content.strip())\n",
    "\n",
    "    # 2. topical_rails.co\n",
    "    rails_content = \"\"\"\n",
    "define user ask about anything else\n",
    "  \"tell me about politics\"\n",
    "  \"what are your thoughts on finance?\"\n",
    "  \"who will win the game?\"\n",
    "\n",
    "define bot refuse to answer\n",
    "  \"I'm sorry, but I can only discuss topics related to HPV vaccination.\"\n",
    "  \"My purpose is to help you practice clinical communication skills for HPV vaccines.\"\n",
    "\n",
    "define flow\n",
    "  user ask about anything else\n",
    "  bot refuse to answer\n",
    "    \"\"\"\n",
    "    with open(\"topical_rails.co\", \"w\") as f:\n",
    "        f.write(rails_content.strip())\n",
    "\n",
    "    print(\"NeMo Guardrails configuration files created.\")\n",
    "\n",
    "create_rails_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a0d11",
   "metadata": {},
   "source": [
    "## 5.0 Multi-Agent Orchestration (LangGraph)\n",
    "Implements the Supervisor-Worker architecture using a state graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18db6a39",
   "metadata": {},
   "source": [
    "### 5.1 Multi-Agent Orchestration Logic\n",
    "\n",
    "This section implements the core reasoning loop using `asyncio` for concurrency. We define three agent classes:\n",
    "- **Supervisor**: Checks input safety using NeMo Guardrails.\n",
    "- **Caregiver**: Generates the persona response (simulating RAG+LLM latency).\n",
    "- **Coach**: Evaluates the turn (simulating C-LEAR rubric latency).\n",
    "\n",
    "The `handle_user_turn` function orchestrates these agents, running the Caregiver and Coach in parallel to minimize response time.\n",
    "\n",
    "\n",
    "![notebook 3 - section 5.png](images/notebook_3_-_section_5.png)\n",
    "\n",
    "Multi-Agent Orchestration (LangGraph): This is the core logic of the backend. It visualizes the Supervisor-Worker pattern. The User Input is first checked by the Supervisor (Guardrails). If safe, it triggers the Caregiver (generating the response) and the Coach (evaluating the response) in parallel to minimize latency. The results are aggregated into a single JSON response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ce485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Any, Dict\n",
    "# from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "# 3.3 Multi-Agent System (MAS) Orchestration Logic\n",
    "\n",
    "class SupervisorAgent:\n",
    "    async def process_input(self, text: str):\n",
    "        # Call NeMo Guardrails here\n",
    "        print(f\"SUPERVISOR: Checking input '{text}'\")\n",
    "        is_safe = \"politics\" not in text.lower()  # Mock check\n",
    "        if is_safe:\n",
    "            return text, True\n",
    "        return \"I cannot discuss that topic.\", False\n",
    "\n",
    "class CaregiverAgent:\n",
    "    async def generate_response(self, text: str):\n",
    "        # RAG + LLM Inference\n",
    "        await asyncio.sleep(0.8)\n",
    "        return f\"Caregiver response to: {text}\"\n",
    "\n",
    "class CoachAgent:\n",
    "    async def evaluate_turn(self, text: str):\n",
    "        # C-LEAR Rubric\n",
    "        await asyncio.sleep(0.4)\n",
    "        return \"Good empathy.\"\n",
    "\n",
    "async def handle_user_turn(user_transcript: str, supervisor, caregiver, coach):\n",
    "    # 1. Supervisor Check\n",
    "    sanitized_text, is_safe = await supervisor.process_input(user_transcript)\n",
    "    if not is_safe:\n",
    "        return sanitized_text\n",
    "\n",
    "    # 2. Parallel Execution\n",
    "    caregiver_task = asyncio.create_task(caregiver.generate_response(sanitized_text))\n",
    "    coach_task = asyncio.create_task(coach.evaluate_turn(sanitized_text))\n",
    "\n",
    "    caregiver_response, coach_feedback = await asyncio.gather(caregiver_task, coach_task)\n",
    "\n",
    "    final_response = f\"{caregiver_response} [Feedback: {coach_feedback}]\"\n",
    "    return final_response\n",
    "\n",
    "class AsyncOrchestrationGraph:\n",
    "    \"\"\"\n",
    "    Minimal async graph adapter to provide an app_graph.ainvoke(...) interface.\n",
    "    This preserves a clear initialization lifecycle without requiring notebook-wide\n",
    "    LangGraph compilation for the prototype.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, supervisor: SupervisorAgent, caregiver: CaregiverAgent, coach: CoachAgent):\n",
    "        self.supervisor = supervisor\n",
    "        self.caregiver = caregiver\n",
    "        self.coach = coach\n",
    "\n",
    "    async def ainvoke(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        transcript = state.get(\"transcript\", \"\")\n",
    "        if not isinstance(transcript, str) or not transcript.strip():\n",
    "            return {\n",
    "                \"final_response\": {\"text\": \"No transcript provided.\", \"audio\": \"\", \"cues\": {}},\n",
    "                \"feedback\": \"\",\n",
    "            }\n",
    "\n",
    "        final_text = await handle_user_turn(\n",
    "            transcript,\n",
    "            self.supervisor,\n",
    "            self.caregiver,\n",
    "            self.coach,\n",
    "        )\n",
    "\n",
    "        caregiver_text = final_text\n",
    "        coach_feedback = \"\"\n",
    "        if \" [Feedback: \" in final_text and final_text.endswith(\"]\"):\n",
    "            caregiver_text, feedback_tail = final_text.rsplit(\" [Feedback: \", 1)\n",
    "            coach_feedback = feedback_tail[:-1]\n",
    "\n",
    "        return {\n",
    "            \"final_response\": {\n",
    "                \"text\": caregiver_text,\n",
    "                \"audio\": \"\",\n",
    "                \"cues\": {\"gesture\": \"speaking\"},\n",
    "            },\n",
    "            \"feedback\": coach_feedback,\n",
    "        }\n",
    "\n",
    "\n",
    "def build_app_graph() -> AsyncOrchestrationGraph:\n",
    "    \"\"\"Canonical orchestrator construction lifecycle for the backend endpoint.\"\"\"\n",
    "    supervisor = SupervisorAgent()\n",
    "    caregiver = CaregiverAgent()\n",
    "    coach = CoachAgent()\n",
    "    return AsyncOrchestrationGraph(supervisor, caregiver, coach)\n",
    "\n",
    "\n",
    "# Example Run\n",
    "# app_graph = build_app_graph()\n",
    "# asyncio.run(app_graph.ainvoke({\"transcript\": \"User said something about vaccines\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f897c430",
   "metadata": {},
   "source": [
    "## 6.0 API Server (FastAPI)\n",
    "Exposes the Orchestrator to the Unity Client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471867cb",
   "metadata": {},
   "source": [
    "### 6.1 FastAPI Server Implementation\n",
    "\n",
    "This cell wraps the orchestration logic in a **FastAPI** application to expose it to the Unity client.\n",
    "- **`/v1/chat` Endpoint**: Accepts a user transcript and session ID, invokes the orchestration loop, and returns the multi-agent response (Text, Audio, Feedback).\n",
    "- **Redacted Audit Logging**: Writes only compliant metadata (`session_id`, `agent_type`, `is_safe`, `latency_ms`, timestamp) and excludes raw transcript content.\n",
    "- **Health Check**: A simple `GET /health` endpoint for monitoring service uptime and audit retention metadata.\n",
    "\n",
    "![notebook 3 - section 6.png](images/notebook_3_-_section_6.png)\n",
    "\n",
    "API Server Integration: This diagram maps the data flow through the FastAPI application. The Unity Client sends a request to /v1/chat. The server invokes the orchestration loop (defined in Section 5), writes redacted audit metadata only, and returns the structured ChatResponse containing text, audio (Base64), and animation cues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# 6.1 Configuration & Logging\n",
    "BASE_PATH = os.environ.get(\"SPARC_BASE_PATH\", \"/blue/jasondeanarnold/SPARCP\")\n",
    "LOG_FILE = os.environ.get(\"SPARC_AUDIT_LOG\", os.path.join(BASE_PATH, \"logs\", \"audit.log\"))\n",
    "AUDIT_RETENTION_DAYS = int(os.environ.get(\"SPARC_AUDIT_RETENTION_DAYS\", \"30\"))\n",
    "os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)\n",
    "logging.basicConfig(filename=LOG_FILE, level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "app_graph = None\n",
    "\n",
    "\n",
    "def log_redacted_audit_event(session_id: str, agent_type: str, is_safe: bool, latency_ms: float):\n",
    "    event = {\n",
    "        \"event\": \"chat_turn\",\n",
    "        \"event_ts\": datetime.now(timezone.utc).isoformat(),\n",
    "        \"session_id\": session_id,\n",
    "        \"agent_type\": agent_type,\n",
    "        \"is_safe\": is_safe,\n",
    "        \"latency_ms\": round(latency_ms, 2),\n",
    "        \"retention_days\": AUDIT_RETENTION_DAYS,\n",
    "    }\n",
    "    logging.info(json.dumps(event, sort_keys=True))\n",
    "\n",
    "\n",
    "def initialize_orchestrator():\n",
    "    \"\"\"Build and inject the orchestrator graph once at startup/init time.\"\"\"\n",
    "    global app_graph\n",
    "    try:\n",
    "        app_graph = build_app_graph()\n",
    "    except Exception as exc:\n",
    "        app_graph = None\n",
    "        logging.error(f\"Failed to initialize orchestrator graph: {exc}\")\n",
    "\n",
    "initialize_orchestrator()\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    session_id: str\n",
    "    user_transcript: str\n",
    "\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    caregiver_text: str\n",
    "    caregiver_audio_b64: str\n",
    "    caregiver_animation_cues: dict\n",
    "    coach_feedback: str\n",
    "\n",
    "\n",
    "# 6.2 Endpoints\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    orchestrator_ready = app_graph is not None and hasattr(app_graph, \"ainvoke\")\n",
    "    return {\n",
    "        \"status\": \"ok\" if orchestrator_ready else \"degraded\",\n",
    "        \"service\": \"SPARC-P Backend\",\n",
    "        \"orchestrator_ready\": orchestrator_ready,\n",
    "        \"audit_log_path\": LOG_FILE,\n",
    "        \"audit_retention_days\": AUDIT_RETENTION_DAYS,\n",
    "    }\n",
    "\n",
    "\n",
    "@app.post(\"/v1/chat\", response_model=ChatResponse)\n",
    "async def chat_endpoint(request: ChatRequest):\n",
    "    # Fail-fast for uninitialized orchestration\n",
    "    if app_graph is None or not hasattr(app_graph, \"ainvoke\"):\n",
    "        raise HTTPException(status_code=503, detail=\"Orchestrator is not initialized\")\n",
    "\n",
    "    # Invoke orchestrator\n",
    "    start_time = time.perf_counter()\n",
    "    initial_state = {\n",
    "        \"transcript\": request.user_transcript,\n",
    "        \"history\": [],\n",
    "        \"feedback\": \"\",\n",
    "        \"next_action\": \"\",\n",
    "        \"final_response\": {},\n",
    "    }\n",
    "    result = await app_graph.ainvoke(initial_state)\n",
    "    latency_ms = (time.perf_counter() - start_time) * 1000\n",
    "\n",
    "    response_data = result.get(\"final_response\", {})\n",
    "    caregiver_text = response_data.get(\"text\", \"Error\")\n",
    "\n",
    "    # Redacted audit log only (no raw transcript / PHI content)\n",
    "    is_safe = caregiver_text != \"I cannot discuss that topic.\"\n",
    "    log_redacted_audit_event(\n",
    "        session_id=request.session_id,\n",
    "        agent_type=\"orchestrator\",\n",
    "        is_safe=is_safe,\n",
    "        latency_ms=latency_ms,\n",
    "    )\n",
    "\n",
    "    return ChatResponse(\n",
    "        caregiver_text=caregiver_text,\n",
    "        caregiver_audio_b64=response_data.get(\"audio\", \"\"),\n",
    "        caregiver_animation_cues=response_data.get(\"cues\", {}),\n",
    "        coach_feedback=result.get(\"feedback\", \"\"),\n",
    "    )\n",
    "\n",
    "# To run:\n",
    "# uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4effd18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Orchestrator Smoke Tests (FastAPI TestClient)\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "# A) Health endpoint should reflect orchestrator readiness\n",
    "health = client.get(\"/health\")\n",
    "print(\"Health:\", health.status_code, health.json())\n",
    "\n",
    "# B) Chat endpoint should succeed when orchestrator is initialized\n",
    "ok_payload = {\"session_id\": \"smoke-session\", \"user_transcript\": \"Can you help me talk about HPV vaccines?\"}\n",
    "ok_response = client.post(\"/v1/chat\", json=ok_payload)\n",
    "print(\"Chat (ready):\", ok_response.status_code, ok_response.json())\n",
    "\n",
    "# C) Chat endpoint should fail-fast when orchestrator is unavailable\n",
    "saved_graph = app_graph\n",
    "app_graph = None\n",
    "degraded_response = client.post(\"/v1/chat\", json=ok_payload)\n",
    "print(\"Chat (degraded):\", degraded_response.status_code, degraded_response.json())\n",
    "\n",
    "# Restore state for subsequent cells\n",
    "app_graph = saved_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94325d84",
   "metadata": {},
   "source": [
    "## 7.0 Security and Compliance\n",
    "**HIPAA Mandate**: This system uses a 'Transient PHI' model. User audio and transcripts are processed in-memory and discarded immediately after the conversational turn. No PHI is written to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab23b24",
   "metadata": {},
   "source": [
    "### 7.1 Production Deployment Script\n",
    "\n",
    "To deploy this backend as a persistent service on HiPerGator, we generate a SLURM script (`launch_backend.slurm`). This script:\n",
    "- Uses your available **4 GPUs and 16 CPU cores** for parallelizable service capacity.\n",
    "- Loads `conda` and `apptainer`.\n",
    "- Launches Riva + FastAPI backend for persistent service execution.\n",
    "\n",
    "![notebook 3 - section 7.png](images/notebook_3_-_section_7.png)\n",
    "\n",
    "Security and Compliance: This section outlines the security protocols and persistent deployment. It adheres to the HIPAA Mandate using a 'Transient PHI' model, where user data is processed in-memory and immediately discarded. The launch_backend.slurm script ensures the service runs persistently on a secure GPU node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77803f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 SLURM Launch Script Generator (Conda-based)\n",
    "import os\n",
    "\n",
    "def generate_launch_script():\n",
    "    \"\"\"\n",
    "    Generates a SLURM script for persistent backend deployment using conda.\n",
    "    Resource profile: 4 GPUs and 16 CPU cores for parallelization.\n",
    "    \"\"\"\n",
    "    script_content = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name=sparcp-backend\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=${SPARC_SLURM_EMAIL:-YOUR_EMAIL@ufl.edu}\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --qos=jasondeanarnold-b\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --gpus-per-task=1\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=128gb\n",
    "#SBATCH --time=7-00:00:00\n",
    "#SBATCH --output=backend_%j.log\n",
    "#SBATCH --error=backend_%j.err\n",
    "\n",
    "pwd; hostname; date\n",
    "\n",
    "echo \"=== SPARC-P Backend Service Launch ===\"\n",
    "\n",
    "echo \"Resource profile: 4 GPUs, 16 CPU cores allocated\"\n",
    "\n",
    "# 1. Load required modules\n",
    "module purge\n",
    "module load conda\n",
    "module load cuda/12.8\n",
    "module load apptainer\n",
    "\n",
    "# 2. Resolve runtime paths from environment\n",
    "SPARC_BASE_PATH=${SPARC_BASE_PATH:-/blue/jasondeanarnold/SPARCP}\n",
    "CONDA_ENV=${SPARC_BACKEND_ENV:-$SPARC_BASE_PATH/conda_envs/sparc_backend}\n",
    "RIVA_SIF=${SPARC_RIVA_SIF:-$SPARC_BASE_PATH/containers/riva_server.sif}\n",
    "BACKEND_WORKDIR=${SPARC_BACKEND_WORKDIR:-$SPARC_BASE_PATH/backend}\n",
    "\n",
    "echo \"Using SPARC_BASE_PATH=$SPARC_BASE_PATH\"\n",
    "echo \"Activating conda environment: $CONDA_ENV\"\n",
    "conda activate $CONDA_ENV\n",
    "\n",
    "# 3. Verify environment\n",
    "echo \"Python: $(which python)\"\n",
    "python -c \"import fastapi, langgraph, transformers; print('✓ Backend packages loaded')\"\n",
    "\n",
    "# 4. Launch Riva container in background\n",
    "echo \"Starting Riva server...\"\n",
    "apptainer exec --nv $RIVA_SIF riva_start.sh &\n",
    "RIVA_PID=$!\n",
    "sleep 30  # Wait for Riva to initialize\n",
    "\n",
    "# 5. Start FastAPI backend\n",
    "echo \"Starting FastAPI backend...\"\n",
    "cd $BACKEND_WORKDIR\n",
    "uvicorn main:app --host 0.0.0.0 --port 8000 --workers 2\n",
    "\n",
    "# Cleanup on exit\n",
    "kill $RIVA_PID\n",
    "echo \"Backend service stopped.\"\n",
    "date\n",
    "\"\"\"\n",
    "    with open(\"launch_backend.slurm\", \"w\") as f:\n",
    "        f.write(script_content.strip())\n",
    "    print(\"✓ Generated launch_backend.slurm\")\n",
    "    print(\"\\nIMPORTANT: Update SPARC_SLURM_EMAIL if needed\")\n",
    "    print(\"\\nSubmit with: sbatch launch_backend.slurm\")\n",
    "\n",
    "generate_launch_script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
